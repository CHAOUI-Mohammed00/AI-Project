{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHAOUI-Mohammed00/AI-Project/blob/master/NLP_Exercices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKNN1-LggNmr"
      },
      "source": [
        "Exercice 1 : Import nltk and download the ‘stopwords’ and ‘punkt’ packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktTxRRPSgIaY",
        "outputId": "73c5b4c6-afad-4aba-e264-5b93bf2f1e9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEeQ2lN0hsPO"
      },
      "source": [
        "Exercice 2:Import spacy and load the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2OhhZaVoexc",
        "outputId": "0c74953a-70d9-4759-c27a-658d196463dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f1e8092f890>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\") \n",
        "nlp #call the variable to examine the object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2xyrjhh4N_t"
      },
      "source": [
        "Exercice 3 : How to tokenize a given text?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLdeSvAq5_pO"
      },
      "source": [
        "with nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO4_MZ5F4XAJ",
        "outputId": "33e51aa8-54d6-4391-fc1b-d32e9b384708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Last', 'week', ',', 'the', 'University', 'of', 'Cambridge', 'shared', 'its', 'own', 'research', 'that', 'shows', 'if', 'everyone', 'wears', 'a', 'mask', 'outside', 'home', ',', 'dreaded', '‘', 'second', 'wave', '’', 'of', 'the', 'pandemic', 'can', 'be', 'avoided', '.']\n"
          ]
        }
      ],
      "source": [
        "text=\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\"\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bEMhiKE9RAV"
      },
      "source": [
        "with spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu19hm1Z9ZIC",
        "outputId": "56130cc0-3324-4a1b-c31a-948aba7c2007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last\n",
            "week\n",
            ",\n",
            "the\n",
            "University\n",
            "of\n",
            "Cambridge\n",
            "shared\n",
            "its\n",
            "own\n",
            "research\n",
            "that\n",
            "shows\n",
            "if\n",
            "everyone\n",
            "wears\n",
            "a\n",
            "mask\n",
            "outside\n",
            "home\n",
            ",\n",
            "dreaded\n",
            "‘\n",
            "second\n",
            "wave\n",
            "’\n",
            "of\n",
            "the\n",
            "pandemic\n",
            "can\n",
            "be\n",
            "avoided\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "text=\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\"\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(text)\n",
        "for i in doc:\n",
        "  print(i.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYGanv9qBqOc"
      },
      "source": [
        "Exercice 4 : How to get the sentences of a text document ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX_EEMofByZE",
        "outputId": "44f7a96c-f585-4ec3-c36f-5e19c189ffed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives.\n",
            " \n",
            "Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others.\n",
            " \n",
            "Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels?\n",
            " \n",
            "Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves.\n",
            " \n",
            "This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens.\n",
            " \n",
            "Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication?\n",
            " \n"
          ]
        }
      ],
      "source": [
        "text=\"\"\"The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
        "doc=nlp(text)\n",
        "for sent in doc.sents:\n",
        "  print(sent.text)\n",
        "  print(' ')\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNHXrx4yEss-"
      },
      "source": [
        "Exercice 5 : How to tokenize a text using the `transformers` package ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "MHykt-hdEykr",
        "outputId": "5d35ff77-d84d-4d36-97c3-41543ed53eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "[101, 1045, 2293, 3500, 2161, 1012, 1045, 2175, 13039, 2007, 2026, 2814, 102]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] i love spring season. i go hiking with my friends [SEP]'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=\"I love spring season. I go hiking with my friends\"\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "inputs=tokenizer.encode(text)\n",
        "print(inputs)\n",
        "tokenizer.decode(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj1rGfzUMTJD"
      },
      "source": [
        "Exercice 6 :  How to tokenize text with stopwords as delimiters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7x7ZbxPMZEl",
        "outputId": "1eacd6b3-a512-4ca9-a3df-6008c2f10cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Walter', 'feeling', 'anxious', '.', 'He', 'diagnosed', 'today', '.', 'He', 'probably', 'best', 'person', 'I', 'know', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "text = \"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\n",
        "stopwords = set(stopwords.words('english'))\n",
        "words = word_tokenize(text)\n",
        "wordsFiltered = []\n",
        "for w in words :\n",
        "  if w not in stopwords:\n",
        "    wordsFiltered.append(w)\n",
        "\n",
        "print(wordsFiltered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocyPu4OrR4lL"
      },
      "source": [
        "Exercice 7 : How to remove stop words in a text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "sNEM4a30R_Gk",
        "outputId": "879df65b-aaa4-473b-dada-263ca762de91"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'outbreak coronavirus disease 2019 ( COVID-19 ) created global health crisis deep impact way perceive world everyday lives . Not rate contagion patterns transmission threatens sense agency , safety measures put place contain spread virus also require social distancing refraining inherently human , find solace company others . Within context physical threat , social physical distancing , well public alarm , ( ) role different mass media channels lives individual , social societal levels ? Mass media long recognized powerful forces shaping experience world . This recognition accompanied growing volume research , closely follows footsteps technological transformations ( e.g . radio , movies , television , internet , mobiles ) zeitgeist ( e.g . cold war , 9/11 , climate change ) attempt map mass media major impacts perceive , individuals citizens . Are media ( broadcast digital ) still able convey sense unity reaching large audiences , messages lost noisy crowd mass self-communication ?'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=\"the outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\n",
        "from nltk.corpus import stopwords\n",
        "my_stopwords=set(stopwords.words('english'))\n",
        "new_tokens=[]\n",
        "\n",
        "all_tokens=nltk.word_tokenize(text)\n",
        "\n",
        "for token in all_tokens:\n",
        "  if token not in my_stopwords:\n",
        "    new_tokens.append(token)\n",
        "\n",
        "\n",
        "\" \".join(new_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW1Ls9rjZNyG"
      },
      "source": [
        "Exercice 8 : How to add custom stop words in spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n47OVLsT7rXK",
        "outputId": "c2f3af31-acb1-49d0-9d19-bb9f2daa1928"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  Jonas great guy Adam evil Martha fool'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=\" Jonas was a JUNK great guy NIL Adam was evil NIL Martha JUNK was more of a fool \"\n",
        "\n",
        "customize_stop_words = ['NIL','JUNK']\n",
        "\n",
        "\n",
        "for w in customize_stop_words:\n",
        "    nlp.vocab[w].is_stop = True\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "\" \".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcoQtXmT9xtI"
      },
      "source": [
        "Exercice 9 : How to remove punctuations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwAF_Xt596GV",
        "outputId": "9c7df263-8a94-4772-f382-9cec8598f3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The match has concluded     India has won the match   Will we fin the finals too    \n"
          ]
        }
      ],
      "source": [
        "text=\"The match has concluded !!! India has won the match . Will we fin the finals too ? !\"\n",
        "import string\n",
        "for char in text:\n",
        "  if char in string.punctuation:\n",
        "    text = text.replace(char,' ')\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJAhchN5ThKl"
      },
      "source": [
        "Exercice 10 How to perform stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tXX8DBnNTkAe",
        "outputId": "57e9d299-6b7b-4032-94de-e7d8f447c604"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'danc is an art . student should be taught danc as a subject in school . i danc in mani of my school function . some peopl are alway hesit to danc .'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "stemmer=PorterStemmer()\n",
        "stemmed_tokens=[]\n",
        "for token in nltk.word_tokenize(text):\n",
        "  stemmed_tokens.append(stemmer.stem(token))\n",
        "\n",
        "\" \".join(stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFCXUTHrcr_u"
      },
      "source": [
        "Exercice 11 : How to lemmatize a given text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A6n090s0cwO7",
        "outputId": "1d285c81-960c-482c-fb66-0c23ea898efb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dancing be an art . student should be teach dance as a subject in school . I dance in many of my school function . some people be always hesitate to dance .'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(text)\n",
        "\n",
        "lemmatized=[token.lemma_ for token in doc]\n",
        "\" \".join(lemmatized)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iftOtwSqf8H7"
      },
      "source": [
        "Exercice 12 : How to extract usernames from emails "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQRI0hjGgCiq",
        "outputId": "9af24bde-70c4-4299-d074-a765fa88caa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['potter709', 'elixir101', 'granger111', 'severus77']\n"
          ]
        }
      ],
      "source": [
        "text= \"The new registrations are potter709@gmail.com , elixir101@gmail.com. If you find any disruptions, kindly contact granger111@gamil.com or severus77@gamil.com \"\n",
        "\n",
        "import re  \n",
        "\n",
        "usernames= re.findall('(\\S+)@', text)     \n",
        "print(usernames) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBRYcx_9lE46"
      },
      "source": [
        "Exercice 13 : How to find the most common words in the text excluding stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFVn-iOV_f3Y",
        "outputId": "c766e8a0-a1b2-44e7-bf33-dcc8cebd07c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{Junkfood: 1,\n",
              " Food: 1,\n",
              " good: 1,\n",
              " body: 1,\n",
              " need: 1,\n",
              " body: 1,\n",
              " willingly: 1,\n",
              " eat: 1,\n",
              " great: 1,\n",
              " taste: 1,\n",
              " easy: 1,\n",
              " cook: 1,\n",
              " ready: 1,\n",
              " eat: 1,\n",
              " Junk: 1,\n",
              " foods: 1,\n",
              " nutritional: 1,\n",
              " value: 1,\n",
              " irrespective: 1,\n",
              " way: 1,\n",
              " marketed: 1,\n",
              " healthy: 1,\n",
              " consume: 1,\n",
              " reason: 1,\n",
              " gaining: 1,\n",
              " popularity: 1,\n",
              " increased: 1,\n",
              " trend: 1,\n",
              " consumption: 1,\n",
              " : 1,\n",
              " ready: 1,\n",
              " eat: 1,\n",
              " easy: 1,\n",
              " cook: 1,\n",
              " foods: 1,\n",
              " People: 1,\n",
              " age: 1,\n",
              " groups: 1,\n",
              " moving: 1,\n",
              " Junkfood: 1,\n",
              " hassle: 1,\n",
              " free: 1,\n",
              " ready: 1,\n",
              " grab: 1,\n",
              " eat: 1,\n",
              " Cold: 1,\n",
              " drinks: 1,\n",
              " chips: 1,\n",
              " noodles: 1,\n",
              " pizza: 1,\n",
              " burgers: 1,\n",
              " French: 1,\n",
              " fries: 1,\n",
              " etc: 1,\n",
              " examples: 1,\n",
              " great: 1,\n",
              " variety: 1,\n",
              " junk: 1,\n",
              " food: 1,\n",
              " available: 1,\n",
              " market: 1,\n",
              " \n",
              "  : 1,\n",
              " Junkfood: 1,\n",
              " dangerous: 1,\n",
              " food: 1,\n",
              " pleasure: 1,\n",
              " eating: 1,\n",
              " gives: 1,\n",
              " great: 1,\n",
              " taste: 1,\n",
              " mouth: 1,\n",
              " examples: 1,\n",
              " Junkfood: 1,\n",
              " kurkure: 1,\n",
              " chips: 1,\n",
              " cold: 1,\n",
              " rings: 1,\n",
              " source: 1,\n",
              " junk: 1,\n",
              " food: 1,\n",
              " shud: 1,\n",
              " nt: 1,\n",
              " ate: 1,\n",
              " high: 1,\n",
              " amounts: 1,\n",
              " results: 1,\n",
              " fatal: 1,\n",
              " body: 1,\n",
              " cn: 1,\n",
              " eated: 1,\n",
              " limited: 1,\n",
              " extend: 1,\n",
              " research: 1,\n",
              " found: 1,\n",
              " tht: 1,\n",
              " ths: 1,\n",
              " junk: 1,\n",
              " foods: 1,\n",
              " r: 1,\n",
              " dangerous: 1,\n",
              " fr: 1,\n",
              " health: 1,\n",
              " : 1,\n",
              " Junkfood: 1,\n",
              " harmful: 1,\n",
              " slowly: 1,\n",
              " eating: 1,\n",
              " away: 1,\n",
              " health: 1,\n",
              " present: 1,\n",
              " generation: 1,\n",
              " term: 1,\n",
              " denotes: 1,\n",
              " dangerous: 1,\n",
              " bodies: 1,\n",
              " importantly: 1,\n",
              " tastes: 1,\n",
              " good: 1,\n",
              " people: 1,\n",
              " consume: 1,\n",
              " daily: 1,\n",
              " basis: 1,\n",
              " awareness: 1,\n",
              " spread: 1,\n",
              " harmful: 1,\n",
              " effects: 1,\n",
              " Junkfood: 1,\n",
              " : 1,\n",
              " problem: 1,\n",
              " think: 1,\n",
              " studies: 1,\n",
              " Junkfood: 1,\n",
              " impacts: 1,\n",
              " health: 1,\n",
              " negatively: 1,\n",
              " contain: 1,\n",
              " higher: 1,\n",
              " levels: 1,\n",
              " calories: 1,\n",
              " fats: 1,\n",
              " sugar: 1,\n",
              " contrary: 1,\n",
              " low: 1,\n",
              " amounts: 1,\n",
              " healthy: 1,\n",
              " nutrients: 1,\n",
              " lack: 1,\n",
              " dietary: 1,\n",
              " fibers: 1,\n",
              " Parents: 1,\n",
              " discourage: 1,\n",
              " children: 1,\n",
              " consuming: 1,\n",
              " junk: 1,\n",
              " food: 1,\n",
              " ill: 1,\n",
              " effects: 1,\n",
              " health: 1,\n",
              " : 1,\n",
              " Junkfood: 1,\n",
              " easiest: 1,\n",
              " way: 1,\n",
              " gain: 1,\n",
              " unhealthy: 1,\n",
              " weight: 1,\n",
              " fats: 1,\n",
              " sugar: 1,\n",
              " food: 1,\n",
              " makes: 1,\n",
              " gain: 1,\n",
              " weight: 1,\n",
              " rapidly: 1,\n",
              " healthy: 1,\n",
              " weight: 1,\n",
              " fats: 1,\n",
              " cholesterol: 1,\n",
              " harmful: 1,\n",
              " impact: 1,\n",
              " health: 1,\n",
              " Junk: 1,\n",
              " food: 1,\n",
              " main: 1,\n",
              " reasons: 1,\n",
              " increase: 1,\n",
              " obesity: 1,\n",
              " nowadays: 1,\n",
              " : 1,\n",
              " food: 1,\n",
              " looks: 1,\n",
              " tastes: 1,\n",
              " good: 1,\n",
              " positive: 1,\n",
              " points: 1,\n",
              " calorie: 1,\n",
              " body: 1,\n",
              " requires: 1,\n",
              " stay: 1,\n",
              " fit: 1,\n",
              " fulfilled: 1,\n",
              " food: 1,\n",
              " instance: 1,\n",
              " foods: 1,\n",
              " like: 1,\n",
              " French: 1,\n",
              " fries: 1,\n",
              " burgers: 1,\n",
              " candy: 1,\n",
              " cookies: 1,\n",
              " high: 1,\n",
              " amounts: 1,\n",
              " sugar: 1,\n",
              " fats: 1,\n",
              " result: 1,\n",
              " long: 1,\n",
              " term: 1,\n",
              " illnesses: 1,\n",
              " like: 1,\n",
              " diabetes: 1,\n",
              " high: 1,\n",
              " blood: 1,\n",
              " pressure: 1,\n",
              " result: 1,\n",
              " kidney: 1,\n",
              " failure: 1}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=\"\"\"Junkfood - Food that do no good to our body. And there's no need of them in our body but still we willingly eat them because they are great in taste and easy to cook or ready to eat. Junk foods have no or very less nutritional value and irrespective of the way they are marketed, they are not healthy to consume.The only reason of their gaining popularity and increased trend of consumption is \n",
        "that they are ready to eat or easy to cook foods. People, of all age groups are moving towards Junkfood as it is hassle free and often ready to grab and eat. Cold drinks, chips, noodles, pizza, burgers, French fries etc. are few examples from the great variety of junk food available in the market.\n",
        " Junkfood is the most dangerous food ever but it is pleasure in eating and it gives a great taste in mouth examples of Junkfood are kurkure and chips.. cold rings are also source of junk food... they shud nt be ate in high amounts as it results fatal to our body... it cn be eated in a limited extend ... in research its found tht ths junk foods r very dangerous fr our health\n",
        "Junkfood is very harmful that is slowly eating away the health of the present generation. The term itself denotes how dangerous it is for our bodies. Most importantly, it tastes so good that people consume it on a daily basis. However, not much awareness is spread about the harmful effects of Junkfood .\n",
        "The problem is more serious than you think. Various studies show that Junkfood impacts our health negatively. They contain higher levels of calories, fats, and sugar. On the contrary, they have very low amounts of healthy nutrients and lack dietary fibers. Parents must discourage their children from consuming junk food because of the ill effects it has on one’s health.\n",
        "Junkfood is the easiest way to gain unhealthy weight. The amount of fats and sugar in the food makes you gain weight rapidly. However, this is not a healthy weight. It is more of fats and cholesterol which will have a harmful impact on your health. Junk food is also one of the main reasons for the increase in obesity nowadays.\n",
        "This food only looks and tastes good, other than that, it has no positive points. The amount of calorie your body requires to stay fit is not fulfilled by this food. For instance, foods like French fries, burgers, candy, and cookies, all have high amounts of sugar and fats. Therefore, this can result in long-term illnesses like diabetes and high blood pressure. This may also result in kidney failure.\"\"\"\n",
        "# Creating spacy doc of the text\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(text)\n",
        "\n",
        "# Removal of stop words and punctuations\n",
        "words=[token for token in doc if token.is_stop==False and token.is_punct==False]\n",
        "\n",
        "freq_dict={}\n",
        "\n",
        "# Calculating frequency count\n",
        "for word in words:\n",
        "  if word not in freq_dict:\n",
        "    freq_dict[word]=1\n",
        "  else:\n",
        "    freq_dict[word]+=1\n",
        "\n",
        "freq_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKUFsJJnCyeF"
      },
      "source": [
        "Exercice 14 : How to do spell correction in a given text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqt6_v4aC7M2",
        "outputId": "811af637-4f6f-4408-dad2-0ab292c9a40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TextBlob(\"He is a great person. He believes in god\")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=\"He is a gret person. He beleives in bod\"\n",
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "tb = TextBlob(text)\n",
        "tb.correct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfCHlw2XEZRQ"
      },
      "source": [
        "Exercice 15 : How to tokenize tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuPhc7gMEcwq",
        "outputId": "7937a4cd-7c8f-4224-fe1d-a07c575dd4c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Having',\n",
              " 'lots',\n",
              " 'of',\n",
              " 'fun',\n",
              " 'goa',\n",
              " 'vaction',\n",
              " 'summervacation',\n",
              " 'Fancy',\n",
              " 'dinner',\n",
              " 'Beachbay',\n",
              " 'restro']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=\" Having lots of fun #goa #vaction #summervacation. Fancy dinner @Beachbay restro :) \"\n",
        "import re\n",
        "# Cleaning the tweets\n",
        "text=re.sub(r'[^\\w]', ' ', text)\n",
        "\n",
        "# Using nltk's TweetTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer=TweetTokenizer()\n",
        "tokenizer.tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skmk_ISaF3Ek"
      },
      "source": [
        "Exercice 16 : How to extract all the nouns in a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOMqVtC7GIUI",
        "outputId": "e77ec0f9-94db-40da-8e94-66da3c47d135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "James\n",
            "Microsoft\n",
            "manchester\n",
            "flute\n"
          ]
        }
      ],
      "source": [
        "txt=\"James works at Microsoft. She lives in manchester and likes to play the flute\"\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(text)\n",
        "\n",
        "\n",
        "for token in doc:\n",
        "  if token.pos_=='NOUN' or token.pos_=='PROPN' :\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2bCWXDOIDdt"
      },
      "source": [
        "Exercice 17 : How to extract all the pronouns in a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRwuEbpYIIb4",
        "outputId": "75541bce-467a-41b5-fe2f-454ad48b4518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He\n",
            "his\n",
            "He\n",
            "his\n",
            "She\n"
          ]
        }
      ],
      "source": [
        "text=\"John is happy finally. He had landed his dream job finally. He told his mom. She was elated \"\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  if token.pos_=='PRON':\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJUA0a1OI-Cq"
      },
      "source": [
        "Exercice 18 : How to find similarity between two words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjNLY8xxJCsm",
        "outputId": "6958b993-ad42-4bfa-8d8f-b0cbacd6f760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-08-11 14:52:13.958169: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 9.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "similarity between amazing and terrible is 0.7189564046532955\n",
            "similarity between amazing and excellent is 0.6636035062214747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "word1=\"amazing\"\n",
        "word2=\"terrible\"\n",
        "word3=\"excellent\"\n",
        "# Convert words into spacy tokens\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "token1=nlp(word1)\n",
        "token2=nlp(word2)\n",
        "token3=nlp(word3)\n",
        "\n",
        "# Use similarity() function of tokens\n",
        "print('similarity between', word1,'and' ,word2, 'is' ,token1.similarity(token2))\n",
        "print('similarity between', word1,'and' ,word3, 'is' ,token1.similarity(token3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yka3wGlaLiEs"
      },
      "source": [
        "Exercice 19 : How to find similarity between two documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7byUqxvALocE",
        "outputId": "77d0da1f-8c46-4311-831b-3c20b4f78392"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5750832206866936"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1=\"John lives in Canada\"\n",
        "text2=\"James lives in America, though he's not from there\"\n",
        "doc1=nlp(text1)\n",
        "doc2=nlp(text2)\n",
        "doc1.similarity(doc2)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DhllPRlOLKZ"
      },
      "source": [
        "Exercice 20 : How to find the cosine similarity of two documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOuQeSRHOSJF",
        "outputId": "da1887a6-ff6c-4bab-dc06-0eee9e290e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.45584231]\n",
            " [0.45584231 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "text1='Taj Mahal is a tourist place in India'\n",
        "text2='Great Wall of China is a tourist place in china'\n",
        "documents=[text1,text2]\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "vectorizer=CountVectorizer()\n",
        "matrix=vectorizer.fit_transform(documents)\n",
        "\n",
        "\n",
        "doc_term_matrix=matrix.todense()\n",
        "doc_term_matrix\n",
        "\n",
        "\n",
        "df=pd.DataFrame(doc_term_matrix)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity(df,df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2RhY-UURThX"
      },
      "source": [
        "Exercice 21 : How to find soft cosine similarity of documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "B1bdQ60BRYUt",
        "outputId": "b0cea510-3565-4ad4-aa19-30e660144534"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-58e93f4fdf84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msent_6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftcossim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'softcossim' is not defined"
          ]
        }
      ],
      "source": [
        "sent_1 = \"Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), that is made by combining ingredients of meat or vegetables with stock, juice, water, or another liquid. \"\n",
        "\n",
        "sent_2 = \"Noodles are a staple food in many cultures. They are made from unleavened dough which is stretched, extruded, or rolled flat and cut into one of a variety of shapes.\"\n",
        "\n",
        "sent_3 = \"Dosa is a type of pancake from the Indian subcontinent, made from a fermented batter. It is somewhat similar to a crepe in appearance. Its main ingredients are rice and black gram.\"\n",
        "\n",
        "sent_4 = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
        "\n",
        "sent_5 = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
        "\n",
        "sent_6 = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
        "# Prepare a dictionary and a corpus.\n",
        "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
        "\n",
        "# Prepare the similarity matrix\n",
        "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
        "\n",
        "# Convert the sentences into bag-of-words vectors.\n",
        "sent_1 = dictionary.doc2bow(simple_preprocess(doc_trump))\n",
        "sent_2 = dictionary.doc2bow(simple_preprocess(doc_election))\n",
        "sent_3 = dictionary.doc2bow(simple_preprocess(doc_putin))\n",
        "sent_4 = dictionary.doc2bow(simple_preprocess(doc_soup))\n",
        "sent_5 = dictionary.doc2bow(simple_preprocess(doc_noodles))\n",
        "sent_6 = dictionary.doc2bow(simple_preprocess(doc_dosa))\n",
        "\n",
        "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
        "\n",
        "# Compute soft cosine similarity\n",
        "print(softcossim(sent_1, sent_2, similarity_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIG9I-XJbfZY"
      },
      "source": [
        "Exercice 22 :  How to find similar words using pre-trained Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no2CarJQbr7V",
        "outputId": "9359ebcd-adc2-4bb0-c6c7-47f8a95a5d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('incredible', 0.9054000973701477),\n",
              " ('awesome', 0.8282865285873413),\n",
              " ('unbelievable', 0.8201264142990112),\n",
              " ('fantastic', 0.778986930847168),\n",
              " ('phenomenal', 0.7642048001289368),\n",
              " ('astounding', 0.7347068786621094),\n",
              " ('wonderful', 0.7263179421424866),\n",
              " ('unbelieveable', 0.7165080904960632),\n",
              " ('remarkable', 0.7095627188682556),\n",
              " ('marvelous', 0.7015583515167236)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[('incredible', 0.90),\n",
        "('awesome', 0.82),\n",
        "('unbelievable', 0.82),\n",
        "('fantastic', 0.77),\n",
        "('phenomenal', 0.76),\n",
        "('astounding', 0.73),\n",
        "('wonderful', 0.72),\n",
        "('unbelieveable', 0.71),\n",
        "('remarkable', 0.70),\n",
        "('marvelous', 0.70)]\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "word2vec_model300 = api.load('word2vec-google-news-300')\n",
        "\n",
        "\n",
        "\n",
        "word2vec_model300.most_similar('amazing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuneyXTDc7Ui"
      },
      "source": [
        "Exercice 23 : How to compute Word mover distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7iEHjjVh7UG",
        "outputId": "d11fe4ed-679b-4eea-8e3f-b564e346e7f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
            "  del sys.path[0]\n"
          ]
        }
      ],
      "source": [
        "sentence_orange = 'Oranges are my favorite fruit'\n",
        "sent=\"apples are not my favorite\"\n",
        "\n",
        "import gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "model=Word2Vec()\n",
        "\n",
        "sentence_orange = 'Oranges are my favorite fruit'\n",
        "sent=\"apples are not my favorite\"\n",
        "\n",
        "\n",
        "distance = model.wmdistance(sent, sentence_orange)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsyTADiSc9N2"
      },
      "source": [
        "Exercice 24 : How to replace all the pronouns in a text with their respective object names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-n1TggKciod2",
        "outputId": "973db954-6b13-462a-e7eb-4e93ebea5631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neuralcoref\n",
            "  Downloading neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.24.50-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.21.6)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.9.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (8.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.1.0->neuralcoref) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.1.0->neuralcoref) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.1.0->neuralcoref) (5.2.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.1.0->neuralcoref) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=2.1.0->neuralcoref) (7.1.2)\n",
            "Collecting botocore<1.28.0,>=1.27.50\n",
            "  Downloading botocore-1.27.50-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.50->boto3->neuralcoref) (2.8.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.50->boto3->neuralcoref) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.24.50 botocore-1.27.50 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.25.11\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f68d120f1ac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install neuralcoref'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Add it to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/neuralcoref/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spacy.strings.StringStore size changed,\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mneuralcoref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralCoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNEURALCOREF_MODEL_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEURALCOREF_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEURALCOREF_CACHE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mstrings.pxd\u001b[0m in \u001b[0;36minit neuralcoref.neuralcoref\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: spacy.strings.StringStore size changed, may indicate binary incompatibility. Expected 112 from C header, got 64 from PyObject"
          ]
        }
      ],
      "source": [
        "text=\" My sister has a dog and she loves him\"\n",
        "# Import neural coref library\n",
        "!pip install neuralcoref\n",
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "# Add it to the pipeline\n",
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "# Printing the coreferences\n",
        "doc1 = nlp('My sister has a dog. She loves him.')\n",
        "print(doc1._.coref_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Tv5x-kc_Pt"
      },
      "source": [
        "Exercice 25 : How to extract topic keywords using LSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvTbrf2_jPRW",
        "outputId": "815c62ca-7bc2-48c8-8fcd-0883cebf103a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: \n",
            "learn new life travelling country feel things  \n",
            "Topic 1: \n",
            "life cherish diaries let share experience home  \n",
            "Topic 2: \n",
            "feel know time people just regions sure  \n",
            "Topic 3: \n",
            "time especially cherish diaries let share life  \n",
            "Topic 4: \n",
            "cherish diaries let share makes feel know  \n",
            "Topic 5: \n",
            "culture augustine course cultural cultures eyes habits  \n",
            "Topic 6: \n",
            "want experiences life things advantage bad bet  \n",
            "Topic 7: \n",
            "observe feel experiences want skills test know  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "texts= [\"\"\"It's all about travel. I travel a lot.  those who do not travel read only a page.” – said Saint Augustine. He was a great travel person. Travelling can teach you more than any university course. You learn about the culture of the country you visit. If you talk to locals, you will likely learn about their thinking, habits, traditions and history as well.If you travel, you will not only learn about foreign cultures, but about your own as well. You will notice the cultural differences, and will find out what makes your culture unique. After retrurning from a long journey, you will see your country with new eyes.\"\"\",\n",
        "        \"\"\" You can learn a lot about yourself through travelling. You can observe how you feel beeing far from your country. You will find out how you feel about your homeland.You should travel You will realise how you really feel about foreign people. You will find out how much you know/do not know about the world. You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills. You will not be the same person after returning home.During travelling you will meet people that are very different from you. If you travel enough, you will learn to accept and appreciate these differences. Traveling makes you more open and accepting.\"\"\",\n",
        "        \"\"\"Some of my most cherished memories are from the times when I was travelling. If you travel, you can experience things that you could never experience at home. You may see beautiful places and landscapes that do not exist where you live. You may meet people that will change your life, and your thingking. You may try activities that you have never tried before.Travelling will inevitably make you more independent and confident. You will realise that you can cope with a lot of unexpected situations. You will realise that you can survive without all that help that is always available for you at home. You will likely find out that you are much stronger and braver than you have expected.\"\"\",\n",
        "        \"\"\"If you travel, you may learn a lot of useful things. These things can be anything from a new recepie, to a new, more effective solution to an ordinary problem or a new way of creating something.Even if you go to a country where they speak the same language as you, you may still learn some new words and expressions that are only used there. If you go to a country where they speak a different language, you will learn even more.\"\"\",\n",
        "        \"\"\"After arriving home from a long journey, a lot of travellers experience that they are much more motivated than they were before they left. During your trip you may learn things that you will want to try at home as well. You may want to test your new skills and knowledge. Your experiences will give you a lot of energy.During travelling you may experience the craziest, most exciting things, that will eventually become great stories that you can tell others. When you grow old and look back at your life and all your travel experiences, you will realise how much you have done in your life and your life was not in vain. It can provide you with happiness and satisfaction for the rest of your life.\"\"\",\n",
        "        \"\"\"The benefits of travel are not just a one-time thing: travel changes you physically and psychologically. Having little time or money isn't a valid excuse. You can travel for cheap very easily. If you have a full-time job and a family, you can still travel on the weekends or holidays, even with a baby. travel  more is likely to have a tremendous impact on your mental well-being, especially if you're no used to going out of your comfort zone. Trust me: travel more and your doctor will be happy. Be sure to get in touch with your physician, they might recommend some medication to accompany you in your travels, especially if you're heading to regions of the globe with potentially dangerous diseases.\"\"\",\n",
        "        \"\"\"Sure, you probably feel comfortable where you are, but that is just a fraction of the world! If you are a student, take advantage of programs such as Erasmus to get to know more people, experience and understand their culture. Dare traveling to regions you have a skeptical opinion about. I bet that you will change your mind and realize that everything is not so bad abroad.\"\"\",\n",
        "        \"\"\" So, travel makes you cherish life. Let's travel more . Share your travel diaries with us too\"\"\"\n",
        "        ]\n",
        "# Importing the Tf-idf vectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Defining the vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
        "\n",
        "# Transforming the tokens into the matrix form through .fit_transform()\n",
        "matrix= vectorizer.fit_transform(texts)\n",
        "\n",
        "# SVD represent documents and terms in vectors\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "SVD_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
        "SVD_model.fit(matrix)\n",
        "\n",
        "# Getting the terms \n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "# Iterating through each topic\n",
        "for i, comp in enumerate(SVD_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    # sorting the 7 most important terms\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    # printing the terms of a topic\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=' ')\n",
        "    print(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mq2Brr7dBAa"
      },
      "source": [
        "Exercice 26 : How to extract topic Keywords using LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h_f_TOH2kWZa",
        "outputId": "19f99991-17f1-4a67-c9f2-20ec49b270a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.052*\"may\" + 0.035*\"realise\" + 0.035*\"home\" + 0.035*\"never\" + 0.035*\"experience\"'), (1, '0.032*\"If\" + 0.032*\"culture\" + 0.032*\"get\" + 0.032*\"regions\" + 0.032*\"people\"'), (2, '0.076*\"travel\" + 0.030*\"learn\" + 0.027*\"\\'\" + 0.027*\"If\" + 0.023*\"new\"'), (3, '0.062*\"life\" + 0.046*\"may\" + 0.031*\"things\" + 0.031*\"lot\" + 0.031*\"much\"'), (4, '0.056*\"feel\" + 0.037*\"learn\" + 0.037*\"find\" + 0.037*\"travel\" + 0.037*\"travelling\"')]\n"
          ]
        }
      ],
      "source": [
        "texts= [\"\"\"It's all about travel. I travel a lot.  those who do not travel read only a page.” – said Saint Augustine. He was a great travel person. Travelling can teach you more than any university course. You learn about the culture of the country you visit. If you talk to locals, you will likely learn about their thinking, habits, traditions and history as well.If you travel, you will not only learn about foreign cultures, but about your own as well. You will notice the cultural differences, and will find out what makes your culture unique. After retrurning from a long journey, you will see your country with new eyes.\"\"\",\n",
        "        \"\"\" You can learn a lot about yourself through travelling. You can observe how you feel beeing far from your country. You will find out how you feel about your homeland.You should travel You will realise how you really feel about foreign people. You will find out how much you know/do not know about the world. You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills. You will not be the same person after returning home.During travelling you will meet people that are very different from you. If you travel enough, you will learn to accept and appreciate these differences. Traveling makes you more open and accepting.\"\"\",\n",
        "        \"\"\"Some of my most cherished memories are from the times when I was travelling. If you travel, you can experience things that you could never experience at home. You may see beautiful places and landscapes that do not exist where you live. You may meet people that will change your life, and your thingking. You may try activities that you have never tried before.Travelling will inevitably make you more independent and confident. You will realise that you can cope with a lot of unexpected situations. You will realise that you can survive without all that help that is always available for you at home. You will likely find out that you are much stronger and braver than you have expected.\"\"\",\n",
        "        \"\"\"If you travel, you may learn a lot of useful things. These things can be anything from a new recepie, to a new, more effective solution to an ordinary problem or a new way of creating something.Even if you go to a country where they speak the same language as you, you may still learn some new words and expressions that are only used there. If you go to a country where they speak a different language, you will learn even more.\"\"\",\n",
        "        \"\"\"After arriving home from a long journey, a lot of travellers experience that they are much more motivated than they were before they left. During your trip you may learn things that you will want to try at home as well. You may want to test your new skills and knowledge. Your experiences will give you a lot of energy.During travelling you may experience the craziest, most exciting things, that will eventually become great stories that you can tell others. When you grow old and look back at your life and all your travel experiences, you will realise how much you have done in your life and your life was not in vain. It can provide you with happiness and satisfaction for the rest of your life.\"\"\",\n",
        "        \"\"\"The benefits of travel are not just a one-time thing: travel changes you physically and psychologically. Having little time or money isn't a valid excuse. You can travel for cheap very easily. If you have a full-time job and a family, you can still travel on the weekends or holidays, even with a baby. travel  more is likely to have a tremendous impact on your mental well-being, especially if you're no used to going out of your comfort zone. Trust me: travel more and your doctor will be happy. Be sure to get in touch with your physician, they might recommend some medication to accompany you in your travels, especially if you're heading to regions of the globe with potentially dangerous diseases.\"\"\",\n",
        "        \"\"\"Sure, you probably feel comfortable where you are, but that is just a fraction of the world! If you are a student, take advantage of programs such as Erasmus to get to know more people, experience and understand their culture. Dare traveling to regions you have a skeptical opinion about. I bet that you will change your mind and realize that everything is not so bad abroad.\"\"\",\n",
        "        \"\"\" So, travel makes you cherish life. Let's travel more . Share your travel diaries with us too\"\"\"\n",
        "        ]\n",
        "# Import gensim, nltk\n",
        "import gensim\n",
        "from gensim import models, corpora\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Before topic extraction, we remove punctuations and stopwords.\n",
        "my_stopwords=set(stopwords.words('english'))\n",
        "punctuations=['.','!',',',\"You\",\"I\"]\n",
        "\n",
        "# We prepare a list containing lists of tokens of each text\n",
        "all_tokens=[]\n",
        "for text in texts:\n",
        "  tokens=[]\n",
        "  raw=nltk.wordpunct_tokenize(text)\n",
        "  for token in raw:\n",
        "    if token not in my_stopwords:\n",
        "      if token not in punctuations:\n",
        "        tokens.append(token)\n",
        "        all_tokens.append(tokens)\n",
        "\n",
        "# Creating a gensim dictionary and the matrix\n",
        "dictionary = corpora.Dictionary(all_tokens)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in all_tokens]\n",
        "\n",
        "# Building the model and training it with the matrix \n",
        "from gensim.models.ldamodel import LdaModel\n",
        "model = LdaModel(doc_term_matrix, num_topics=5, id2word = dictionary,passes=40)\n",
        "\n",
        "print(model.print_topics(num_topics=6,num_words=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiSmQuQndDi5"
      },
      "source": [
        "Exercice 27 : How to extract topic keywords using NMF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts= [\"\"\"It's all about travel. I travel a lot.  those who do not travel read only a page.” – said Saint Augustine. He was a great travel person. Travelling can teach you more than any university course. You learn about the culture of the country you visit. If you talk to locals, you will likely learn about their thinking, habits, traditions and history as well.If you travel, you will not only learn about foreign cultures, but about your own as well. You will notice the cultural differences, and will find out what makes your culture unique. After retrurning from a long journey, you will see your country with new eyes.\"\"\",\n",
        "        \"\"\" You can learn a lot about yourself through travelling. You can observe how you feel beeing far from your country. You will find out how you feel about your homeland.You should travel You will realise how you really feel about foreign people. You will find out how much you know/do not know about the world. You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills. You will not be the same person after returning home.During travelling you will meet people that are very different from you. If you travel enough, you will learn to accept and appreciate these differences. Traveling makes you more open and accepting.\"\"\",\n",
        "        \"\"\"Some of my most cherished memories are from the times when I was travelling. If you travel, you can experience things that you could never experience at home. You may see beautiful places and landscapes that do not exist where you live. You may meet people that will change your life, and your thingking. You may try activities that you have never tried before.Travelling will inevitably make you more independent and confident. You will realise that you can cope with a lot of unexpected situations. You will realise that you can survive without all that help that is always available for you at home. You will likely find out that you are much stronger and braver than you have expected.\"\"\",\n",
        "        \"\"\"If you travel, you may learn a lot of useful things. These things can be anything from a new recepie, to a new, more effective solution to an ordinary problem or a new way of creating something.Even if you go to a country where they speak the same language as you, you may still learn some new words and expressions that are only used there. If you go to a country where they speak a different language, you will learn even more.\"\"\",\n",
        "        \"\"\"After arriving home from a long journey, a lot of travellers experience that they are much more motivated than they were before they left. During your trip you may learn things that you will want to try at home as well. You may want to test your new skills and knowledge. Your experiences will give you a lot of energy.During travelling you may experience the craziest, most exciting things, that will eventually become great stories that you can tell others. When you grow old and look back at your life and all your travel experiences, you will realise how much you have done in your life and your life was not in vain. It can provide you with happiness and satisfaction for the rest of your life.\"\"\",\n",
        "        \"\"\"The benefits of travel are not just a one-time thing: travel changes you physically and psychologically. Having little time or money isn't a valid excuse. You can travel for cheap very easily. If you have a full-time job and a family, you can still travel on the weekends or holidays, even with a baby. travel  more is likely to have a tremendous impact on your mental well-being, especially if you're no used to going out of your comfort zone. Trust me: travel more and your doctor will be happy. Be sure to get in touch with your physician, they might recommend some medication to accompany you in your travels, especially if you're heading to regions of the globe with potentially dangerous diseases.\"\"\",\n",
        "        \"\"\"Sure, you probably feel comfortable where you are, but that is just a fraction of the world! If you are a student, take advantage of programs such as Erasmus to get to know more people, experience and understand their culture. Dare traveling to regions you have a skeptical opinion about. I bet that you will change your mind and realize that everything is not so bad abroad.\"\"\",\n",
        "        \"\"\" So, travel makes you cherish life. Let's travel more . Share your travel diaries with us too\"\"\"\n",
        "        ]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
        "\n",
        "nmf_matrix= vectorizer.fit_transform(texts)\n",
        "from sklearn.decomposition import NMF\n",
        "nmf_model = NMF(n_components=6)\n",
        "nmf_model.fit(nmf_matrix)\n",
        "\n",
        "\n",
        "def print_topics_nmf(model, vectorizer, top_n=6):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (idx))\n",
        "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
        "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
        "        \n",
        "print_topics_nmf(nmf_model,vectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJqZK2TvVeg",
        "outputId": "f456c45a-a778-45e7-c320-ecffbaff795e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:\n",
            "[('new', 0.6329770846997637), ('learn', 0.4981038982593204), ('speak', 0.4747754621454479), ('language', 0.4344302967047202), ('country', 0.366539098453833), ('things', 0.343322373043906)]\n",
            "Topic 1:\n",
            "[('life', 0.34063551920788593), ('home', 0.3140201464324052), ('experience', 0.3025841622571267), ('realise', 0.24642870225288166), ('travelling', 0.21809155530250618), ('things', 0.20763478958892476)]\n",
            "Topic 2:\n",
            "[('feel', 0.3462484013922368), ('know', 0.2840008818200788), ('people', 0.2431266883545065), ('world', 0.22169277349691893), ('traveling', 0.22169277349691893), ('bet', 0.18671974365540311)]\n",
            "Topic 3:\n",
            "[('time', 0.44163173193053434), ('especially', 0.2944211546203563), ('zone', 0.14721057731017814), ('dangerous', 0.14721057731017814), ('excuse', 0.14721057731017814), ('benefits', 0.14721057731017814)]\n",
            "Topic 4:\n",
            "[('cherish', 0.47037139100175096), ('diaries', 0.47037139100175096), ('share', 0.47037139100175096), ('let', 0.47037139100175096), ('life', 0.35360092337281207), ('makes', 0.341028624534489)]\n",
            "Topic 5:\n",
            "[('learn', 0.27905960011025244), ('culture', 0.22858906607458263), ('country', 0.20863153116871466), ('locals', 0.14082722468160772), ('eyes', 0.14082722468160772), ('retrurning', 0.14082722468160772)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py:294: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr_PjxE1dFve"
      },
      "source": [
        "Exercice 28 :  How to classify a text as positive/negative sentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"It was a very pleasant day\"\n",
        "\n",
        "from textblob import TextBlob\n",
        "blob=TextBlob(text)\n",
        "\n",
        " \n",
        "print(blob.sentiment)\n",
        "if(blob.sentiment.polarity > 0):\n",
        "  print(\"Positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBB79EWGvkEC",
        "outputId": "558f06d6-eff8-4fdd-fb18-dec3dc0eac36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.9533333333333333, subjectivity=1.0)\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzVxau-wdHVP"
      },
      "source": [
        "Exercice 29 : How to use the Word2Vec model for representing words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts= [\" Photography is an excellent hobby to pursue \",\n",
        "        \" Photographers usually develop patience, calmnesss\"\n",
        "        \" You can try Photography with any good mobile too\"]\n",
        "import nltk\n",
        "tokens=[]\n",
        "for text in texts:\n",
        "  tokens=[]\n",
        "  raw = nltk.wordpunct</em>tokenize(text)\n",
        "  for token in raw:\n",
        "    tokens.append(token)\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "model=Word2Vec(all_tokens)\n",
        "\n",
        "model['Photography']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "g9nEBCgmvvOe",
        "outputId": "ca67efd2-2678-4015-df68-06f57ccbcb28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-2e75e5e46811>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    raw = nltk.wordpunct</em>tokenize(text)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hzzvmtXdJAb"
      },
      "source": [
        "Exercice 30 : How to visualize the word embedding obtained from word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts= [\" Photography is an excellent hobby to pursue \",\n",
        "        \" Photographers usually develop patience, calmnesss\"\n",
        "        \" You can try Photography with any good mobile too\"]\n",
        "import nltk\n",
        "all_tokens=[]\n",
        "for text in texts:\n",
        "  tokens=[]\n",
        "  raw=nltk.wordpunct_tokenize(text)\n",
        "  for token in raw:\n",
        "    tokens.append(token)\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "model=Word2Vec(all_tokens)\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "n6lUccAlwAXK",
        "outputId": "3eeaab6c-a99c-4d4a-c160-f3fd4adaa374"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAD8CAYAAACsAHnpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVxV1f74/9dbREVRcQrnxK4zo4BhOJDXobLUTK+a1yFLc0qv/rLo2sDNvFnZ7X6tzLQ0NEvLTM3hml7j5pApICqa5hCVw8ecQEFQhvX7g8PpgAcwgXMY3s/H4zw4e+21117rMLxZa6+9thhjUEoppRyhkrMroJRSquLQoKOUUsphNOgopZRyGA06SimlHEaDjlJKKYfRoKOUUsphiiXoiMh9InJURI6LSLid/VVFZKVl//ci0sKS3ktEYkTkoOVrD5tjAi3px0VknohIcdRVKaWU8xQ56IiIC/AucD/QHhgmIu3zZHscuGyM+RPwFvCaJf0C8JAxxgcYBSyzOeY9YCzQyvK6r6h1VUop5VzF0dPpBBw3xpw0xtwAVgD98+TpD0Ra3q8C/iwiYozZZ4w5Y0k/BLhZekWNgFrGmN0m++7VpcCAYqirUkopJ6pcDGU0AX612T4F3J1fHmNMhogkAfXI7unkeASINcZcF5EmlnJsy2xSWEXq169vWrRo8YcboJRSFVlMTMwFY0wDR5yrOIJOkYlIB7KH3HrfxrHjgHEAzZs3Jzo6uphrp5RS5ZuI/OyocxXH8NppoJnNdlNLmt08IlIZqA1ctGw3Bb4ERhpjTtjkb1pImQAYYxYaY4KMMUENGjgkUCullLpNxRF09gKtRMRLRKoAQ4F1efKsI3uiAMAgYJsxxoiIB7ABCDfG7MzJbIw5C1wRkRDLrLWRwNpiqKtSSjlUYmIi8+fPd3Y1So0iBx1jTAYwGdgM/AB8Zow5JCIvi0g/S7YPgXoichyYDuRMq54M/Al4UUTiLK87LPsmAh8Ax4ETwKai1lUppRxNg05uUp4ebRAUFGT0mo5SqjQZOnQoa9eupU2bNvTq1QuATZs2ISI8//zzDBkyxMk1BBGJMcYEOeJcuiKBUkqVoDlz5nDXXXcRFxdHSEgIcXFx7N+/n61btzJjxgzOnj3r7Co6lAYdpZQqAWv2nSZ0zja6vLaNkxdSWLPvNDt27GDYsGG4uLjg6elJ9+7d2bt3r7Or6lAadJRSqpit2Xea51Yf5HRiKgAZmVk8t/ogJ35LdnLNnE+DjlJKFbM3Nh8lNT0TAKniRtaNVFLTM/lRmrJy5UoyMzM5f/483377LZ06dXJybR2rVNwcqpRS5ckZSw8HwMWtFlWbtOfMhxNxaxnEQ9188fPzQ0R4/fXXadiwoRNr6ng6e00ppYpZ6Jxt1qE1W0083NgZ3sPOEc6ls9eUUqoMm9GnDW6uLrnS3FxdmNGnjZNqVHro8JpSShWzAQHZ6xO/sfkoZxJTaezhxow+bazpFZkGHaWUKgEDAppokLFDh9eUUko5jAYdpZRSDqNBRymllMNo0FFKKeUwGnSUUko5jAYdpZRSDqNBRymllMNo0FFKKeUwGnSUUko5jAYdpZRSDlMsQUdE7hORoyJyXETC7eyvKiIrLfu/F5EWlvR6IvKNiCSLyDt5jomylBlned1RHHVVSinlPEVee01EXIB3gV7AKWCviKwzxhy2yfY4cNkY8ycRGQq8BgwB0oAXAG/LK6/hxhh9VoFSSpUTxdHT6QQcN8acNMbcAFYA/fPk6Q9EWt6vAv4sImKMSTHG7CA7+CillCrniiPoNAF+tdk+ZUmzm8cYkwEkAfVuoewllqG1F0RE7GUQkXEiEi0i0efPn//jtVdKKeUwpXkiwXBjjA/Q1fIaYS+TMWahMSbIGBPUoEEDh1ZQKaXUH1McQec00Mxmu6klzW4eEakM1AYuFlSoMea05etV4BOyh/GUUkqVYcURdPYCrUTES0SqAEOBdXnyrANGWd4PArYZY0x+BYpIZRGpb3nvCjwIxBdDXZVSSjlRkWevGWMyRGQysBlwARYbYw6JyMtAtDFmHfAhsExEjgOXyA5MAIhIAlALqCIiA4DewM/AZkvAcQG2AouKWlellFLOJQV0OMqcoKAgEx2tM6yVUuqPEJEYY0yQI85VmicSKKWUKmc06CillHIYDTpKKaUcRoOOUkoph9Ggo5RSymE06CillHIYDTpKKaUcRoOOUkoph9Ggo5RSymE06CillHIYDTpKKaUcRoOOUkoph9Ggo5RSymE06CillHIYDTpKKaUcRoOOUkoph9Ggo5RSymE06CillHKYYgk6InKfiBwVkeMiEm5nf1URWWnZ/72ItLCk1xORb0QkWUTeyXNMoIgctBwzT0SkOOqqlFLKeYocdETEBXgXuB9oDwwTkfZ5sj0OXDbG/Al4C3jNkp4GvAA8bafo94CxQCvL676i1lUppZRzFUdPpxNw3Bhz0hhzA1gB9M+Tpz8QaXm/CviziIgxJsUYs4Ps4GMlIo2AWsaY3cYYAywFBhRDXZVSSjlRcQSdJsCvNtunLGl28xhjMoAkoF4hZZ4qpEyllFJlTJmfSCAi40QkWkSiz58/7+zqKKWUKkBxBJ3TQDOb7aaWNLt5RKQyUBu4WEiZTQspEwBjzEJjTJAxJqhBgwZ/sOpK/XEJCQl4e3s7uxpKlUnFEXT2Aq1ExEtEqgBDgXV58qwDRlneDwK2Wa7V2GWMOQtcEZEQy6y1kcDaYqirUkopJ6pc1AKMMRkiMhnYDLgAi40xh0TkZSDaGLMO+BBYJiLHgUtkByYARCQBqAVUEZEBQG9jzGFgIvAR4AZssryU+sNmzZrFxx9/TIMGDWjWrBmBgYH07NmT8ePHc+3aNe666y4WL15MnTp1iIuLs5seExPDmDFjAOjdu7eTW6RU2VUs13SMMRuNMa2NMXcZY2Zb0l60BByMMWnGmMHGmD8ZYzoZY07aHNvCGFPXGONujGlqCTgYY6KNMd6WMicX1DNSKj979+7liy++YP/+/WzatIno6GgARo4cyWuvvcaBAwfw8fHhH//4R4Hpjz32GG+//Tb79+93WluUKg/K/EQCpQqyc+dO+vfvT7Vq1ahZsyYPPfQQKSkpJCYm0r17dwBGjRrFt99+S1JSkt30xMREEhMT6datGwAjRoxwWnuUKuuKPLymVGm0Zt9p3th8lB+2HKYGaQTsO82AAJ11r5SzaU9HlTtr9p3mudUHOZ2YStWm7fjt0C6e/SyGT3f+yPr166lRowZ16tRh+/btACxbtozu3btTu3Ztu+keHh54eHiwY8cOAJYvX+60tilV1mlPR5U5iYmJfPLJJ0ycONHu/jc2HyU1PROAqo1a4/anTpx8fwJPrqhLr44+1K5dm8jISOuEgZYtW7JkyRKAfNOXLFnCmDFjEBGdSKBUEUh5uj4fFBRkci4Uq/IrISGBBx98kPj4+FzpGRkZVK5cGa/wDdj+VGfdSKVSFTdMehoN/vcqCxcupGPHjo6ttFKlmIjEGGOCHHEu7emoMic8PJwTJ07g7++Pq6sr1apVo06dOhw5coShQ4dC/EXwfgCAy98uJfVkDJgsKpsMxk+boAFHKSfSoKPKnDlz5hAfH09cXBxRUVH07duX+Ph4vLy8SEhIYPnnD1El4CGu3Ujn2g/f0nDkv3CvVYdXB/roZAKlnEwnEqgyr1OnTnh5eQHQokULWjZtyNj2Qo3fDlHljpY0b+SpAUepUkKDTjGJiIhg7ty5N6WfOXOGQYMGARAVFcWDDz7o6KoxYMAAAgMD6dChAwsXLgTA3d2dmTNn4ufnR0hICOfOnePq1at4eXmRnp4OwJUrV3JtO9uafacJnbONLq9t4+SFFNbsy16Or0aNGrnyPfHEE/y8eyM+1/bx+Vsz2RneQwOOUqWEBp0S1rhxY1atWuXUOixevJiYmBiio6OZN28eFy9eJCUlhZCQEPbv30+3bt1YtGgRNWvWJCwsjA0bNgCwYsUKBg4ciKurq1PrD7mnQUsVN26kpvDc6oPsOHbzyuIPP/ww//nPf9i7dy99+vRxQm2VUvnRoGMjISGBtm3bMnr0aFq3bs3w4cPZunUroaGhtGrVij179nDp0iUGDBiAr68vISEhHDhwwHr8/v376dy5M61atWLRokXWMu2tSJySksKYMWPo1KkTAQEBrF1bcuuZzps3z9qj+fXXXzl27BhVqlSx9roCAwNJSEgAsnsJttOEH3vssRKrl63CVm62nQbt4lYLk3GD4+88xpx/vJArX0REBPPmzePee+/lL3/5Cy4uLiVab6XUH6MTCfI4fvw4n3/+OYsXLyY4OJhPPvmEHTt2sG7dOv75z3/SrFkzAgICWLNmDdu2bWPkyJHExcUBcODAAXbv3k1KSgoBAQH07ds33/PMnj2bHj16sHjxYhITE+nUqRM9e/a8aajoduTcjX8mMZUal46SuXcjMd99R/Xq1QkLCyMtLQ1XV1eyF/AGFxcXMjIyAAgNDSUhIYGoqCgyMzNLzRL+ZxJTc2271mtGnXsfp1qjVqyfk/tzzsrKYvfu3Xz++eeOrKJS6hZU+J5OznUCr/ANPPLeLu5o3AwfHx8qVapEhw4d+POf/4yI4OPjQ0JCAjt27LCuvdWjRw8uXrzIlStXAOjfvz9ubm7Ur1+fe++9lz179uR73q+//po5c+bg7+9vDQS//PJLsbQnZxjKAL9dvMyvKcLXRy9z5MgRdu/eXWgZI0eO5NFHH3VYLydHZmYmY8eOpUOHDvTu3ZvU1FTi4uIICQnht8in+G31K2SmJVvzpxzaxm9Lp+Lt7W39rM+fP8+sWbP47bffeOCBB6w9zpEjR7JmzRrrscOHDy/R3qVSyr4KHXTy/oE+dyWNi2nGeoG6UqVKVK1a1fo+pzeQn5yeQ37btowxfPHFF8TFxREXF8cvv/xCu3btitYgcg9DAbh5BZKZkcmjfe4hPDyckJCQQssYPnw4ly9fZtiwYUWuzx9x7NgxJk2axKFDh/Dw8OCLL76wrvq8fON2qnt6kbTjE2v+SpnpLN/wP+bPn2997ECDBg1o2bIlJ06c4LvvvuPll1/mzJkzPP7443z00UcAJCUlsWvXrgJ7ohXVPffc4+wqqHKuQgedvH+gITsYvLH5aL7HdO3a1br2VlRUFPXr16dWrVoArF27lrS0NC5evEhUVBTBwcH5ltOnTx/efvttclaE2LdvX1GbA9w8DCWVXfH8yz/wHDOfNWvWEBUVRVhYGMnJv/cYBg0aZP2DDLBjxw4GDRqEh4dHsdTpVnl5eeHv7w9kX2c6ceKEddXnAQFNmDVjEllnDyNA1couTB//GAMCmtCtWzeuXLlCYmIiYL/H2b17d44dO8b58+f59NNPeeSRR6hcWUeX89q1a5ezq6DKuQr9W5f3D3Rh6ZB9oXrMmDH4+vpSvXp1IiMjrft8fX259957uXDhAi+88AKNGze2XqDP64UXXuBvf/sbvr6+ZGVl4eXlxfr164vUHoDGHm6ctlP/xh5ut3T8U089xaZNm9i4cWOR61IY22tPdU0S183vF/1dXFysQSRH7w4NWeRZk9g5fQnb/QZdW+d+PHlOzzK/HufIkSP5+OOPWbFihXWyhMrN3d2d5ORkzp49y5AhQ7hy5QoZGRm89957dO3a1dnVU+VAhV57LXTONrt/oJt4uLEzvEdxVs1hcoYMcw2xubqUupsj89YzI+kc5794mZWbdzIgoAlz584lOTmZL7/8knfeeYeuXbsSERFBUlISb731FmFhYbRt25YFCxawY8cOJkyYwMGDB4mIiGDNmjW5JnTs3r2bxo0bc+7cOTp16kTDhg35/vvvnfwJlE45QefNN98kLS2NmTNnkpmZybVr16hZs6azq6dKiK695iAz+rSx+wd6Rp82TqxV0eQElpweRGMPN2b0aVOqAg4UPLRpW9f8Vn0GqFatGgEBAaSnp7N48WJrur0eJ4Cnpyft2rVjwIABJdy6ssW2x5mansmafacJDg5mzJgxpKenM2DAAOuwp1JFVaF7OpD7F660/oEuj/KuBJ1DgJ/mlMwF/mvXruHj40NsbCy1a9cukXOUNXl7nL/8axBtnv2SVwf60MlT2LBhA++++y7Tp09n5MiRTq6tKillrqcjIvcB/w9wAT4wxszJs78qsBQIBC4CQ4wxCZZ9zwGPA5nAFGPMZkt6AnDVkp5RUh/IgIAmGmScoKjXnv6orVu38vjjjzNt2jQNODbs9ThT0zOZteJb9vzzL4wdO5br168TGxurQUcViyIHHRFxAd4FegGngL0iss4Yc9gm2+PAZWPMn0RkKPAaMERE2gNDgQ5AY2CriLQ2xuT8FtxrjLlQ1Dqq0sfRQ5s9e/bk559/LpGyy7L8Js38Er8XP7/ZuLq64u7uztKlSx1cM1VeFUdPpxNw3BhzEkBEVgD9Adug0x+IsLxfBbwj2VOK+gMrjDHXgZ9E5LilvO+KoV6qFCsr157Ku7w9zubTs9cJbN31QXZu+JezqqXKseIIOk2AX222TwF355fHGJMhIklAPUv67jzH5vzVMcDXImKA940xC+2dXETGAeMAmjdvXrSWKIfSoU3nK4+TaVTpVppvDu1ijOkI3A9MEpFu9jIZYxYaY4KMMUENGjSwl0UVk9GjR1tXzA4LC+N2Hw0eFRWlNyGWEgMCmvDqQB+aeLghZN8uUNqm16vypTh6OqeBZjbbTS1p9vKcEpHKQG2yJxTke6wxJufrbyLyJdnDbt8WQ32Vk0VFReHu7q5LrpQS2uNUjlQcPZ29QCsR8RKRKmRPDFiXJ886YJTl/SBgm8meq70OGCoiVUXEC2gF7BGRGiJSE0BEagC9gfhiqGuF8PHHH9OpUyf8/f158skn+f777/H19SUtLY2UlBQ6dOhAfHw8mZmZPP3003h7e+Pr68vbb78NQExMDN27dycwMJA+ffpw9uzZAs/39ddf07lzZzp27MjgwYOtS+y0aNGCl156iY4dO+Lj48ORI0dISEhgwYIFvPXWW/j7+7N9+/YS/zyUUqVHkYOOMSYDmAxsBn4APjPGHBKRl0WknyXbh0A9y0SB6UC45dhDwGdkTzr4DzDJMnPNE9ghIvuBPcAGY8x/ilrXiuCHH35g5cqV7Ny5k7i4OFxcXDh69Cj9+vXj+eef55lnnuGvf/0r3t7eLFy4kISEBOLi4jhw4ADDhw8nPT2dp556ilWrVhETE8OYMWOYOXNmvue7cOECr7zyClu3biU2NpagoCD+9a/fL0DXr1+f2NhYJkyYwNy5c2nRogXjx49n2rRpxMXFVdilVRITE5k/f76zq6GUwxXLfTrGmI3AxjxpL9q8TwMG53PsbGB2nrSTgF9x1K0isL3BVQ5vJnH3Xutio6mpqdxxxx28+OKLBAcHU61aNebNmwdk37syfvx468KXdevWJT4+nvj4eHr16gVkP26gUaNG+Z579+7dHD58mNDQUABu3LhB586drfsHDhwIZC/guXr16uJvfBmVE3QmTpzo7Koo5VAVehmc8iDvHeVJqTeQ1t2JeOdfucbpz549S3JyMunp6aSlpeX7sDhjDB06dOC7725t1roxhl69evHpp5/a3Z/zaAjbB8UpCA8P58SJE/j7+1sD/KZNmxARnn/+eYYMGYIxhmeeeeamdKXKstI8e03dgrx3lFe7048rP2xn9hfZM9EvXbrEzz//zJNPPsmsWbMYPnw4zz77LAC9evXi/ffftwaDS5cu0aZNG86fP28NOunp6Rw6dCjf84eEhLBz506OHz8OZD+G+8cffyywzjVr1uTq1au33+gyxMXFBX9/f7y9vRk8eDDXrl0jISGB2NhY7rrrLutD6uLi4ti/fz9bt25lxowZnD17ltWrV1vTn3jiCaZPn17o9bWS5O7u7rRzq/JDg04Zl/eO8ir1m+PRdQT7F83A19eXXr16ERkZiaurK48++ijh4eHs3buXbdu28cQTT9C8eXN8fX3x8/Pjk08+oUqVKqxatYpnn30WPz8//P39C5ze3KBBAz766COGDRuGr68vnTt35siRIwXW+aGHHuLLL7+sEBMJ3NzciIuLIz4+nipVqjDphdd45L1d/HQhhZMXUliz7zQ7duxg2LBhuLi44OnpSffu3dm7d2+u9C+//BI/Pz/27t1b5Do5o8epvVyVo8Iv+FnWlcfHM5QnOY8KABg/cw4rN++gRvBA/u/TmZi0q1SuWY9alTOZ9dLzjB8/nri4OHr27Im7uzuVK1dm6tSpNGrUiNGjR1OpUiXq1avH4cOH2bVrF08//TQZGRkEBwfz3nvvUbVqVTZu3Mj06dOpUaMGoaGhnDx5kvXr1xMREcGJEyc4efIkzZs359VXX2XEiBGkpKQA8M4773DPPfcQFRXFiy++SM2aNTl+/Dj33nsv8+fPp1KlSri7uzN16lTWr1+Pm5sba9euxdPTk/PnzzN+/Hjr49b//e9/ExoaetM5n3/+eR577DFu3LhBVlYWX3zxBa1atXLa90b9zpELfmKMKTevwMBAU9F8GXvKtH1+k7nz2fXWV9vnN5kvY085u2rKGFOjRg1jjDHp6emmTrt7TN3eE02T8R8aEONSo46589n1xr15e+Pj42MyMjJMu3btjKenpzl79qwZNGiQad68ucnIyDCdO3c2DRs2NGfPnjWpqammadOm5ujRo8YYY0aMGGHeeusta/rJkyeNMcYMHTrU9O3b1xhjzEsvvWQ6duxorl27ZowxJiUlxaSmphpjjPnxxx9Nzu/ON998Y6pWrWpOnDhhMjIyTM+ePc3nn39ujDEGMOvWrTPGGDNjxgwza9YsY4wxw4YNM9u3bzfGGPPzzz+btm3b2j3n5MmTzccff2yMMeb69evWdOV8QLRx0N9pnUhQxukaZqVbamqq9Vk0GW7NqOPbi8zkS1Su05AqDVtz5sOJVKpWk1q1auHj48Px48dZtmwZDRs25NVXXyU0NBQ/Pz8SEhL4+9//TsOGDdm/fz9eXl60bt0agFGjRvHuu+8SFhZGy5Yt8fLyAmDYsGEsXPj76lH9+vXDzS17Fe/09HQmT55snVZvex2uU6dOtGzZ0lpGzuPLq1SpwoMPPghkz0bcsmULkD0L8vDh35davHLlirV3Z3vOzp07M3v2bE6dOsXAgQO1l1NBadApB/SO8tLFdgo7lasQsWQDAwKa5BoKFRdXGvSbkX3Aga/o2akh06ZNw8fHxzpDTURo0qQJsbGxhIWF0bt37yLVy3bG4ltvvYWnpyf79+8nKyuLatWqWffl97hvV1dX63vb2YhZWVns3r07Vxn2zvnoo49y9913s2HDBh544AHef/99evTQIeCKRicSKFWAAQMGEBgYSIcOHay9Bnd3d2bOnImfnx8hISGcO3fOmj9nCvvpxFQMYAw8t/oga/adZkafNri5uuQq383VhbA22WsG1q5dmzp16lgnVyxbtozu3bsDuWf8tWnThoSEBOuMwZx8bdq04eTJkyQkJACwcuXKfNuVlJREo0aNqFSpEsuWLSMz8/cZkHv27OGnn34iKyuLlStX0qVLlwI/o969e1tXswCIi4uzm+/kyZO0bNmSKVOm0L9/fw4cOFBguap80qCjVAEWL15MTEwM0dHRzJs3j4sXL5KSkkJISAj79++nW7duLFq0yJo/v4ei5TyG+9WBPnjWyu4R5Cyu6d3k94fKRUZGMmNG9szDuLg4Xnwx+x7r0aNHM378ePz9/THGsGTJEgYPHoyPjw+VKlVi/PjxuLm5MX/+fO677z4CAwOpWbNmvg+smzhxIpGRkfj5+XHkyJFcPZLg4GAmT55Mu3bt8PLy4uGHHy7wM5o3bx7R0dH4+vrSvn17FixYYDffZ599hre3N/7+/sTHx+tD4Soonb2mVAEiIiL48ssvAUhISGDz5s10796dtLQ0RISVK1eyZcsWPvjgA8A5j+G2lZycjLu7O8YYJk2aRKtWrZg2bdotHx8VFcXcuXNZv359CdZSlTZl7nHVSpUXttdjalw6SubejcR89x3Vq1cnLCyMtLS0fK9tgOMfw53XokWLiIyM5MaNGwQEBPDkk0865LxK3SoNOkpZ5F1S6LeLl7mWInx99DJt3X5h9+7dhZTg/IeiTZs27Q/1bPIKCwsjLCys+CqkVB4adJSyyHs9xs0rkKv7NvFon3vofU8AISEhhZahU9iVKphe01HKwtnXY5RyFkde09HZa0pZ5HfdxVHXY5SqCDToKGWR3300jroeo1RFoNd0lLLQ6zFKlTwNOkrZ0CWFlCpZxTK8JiL3ichRETkuIuF29lcVkZWW/d+LSAubfc9Z0o+KSJ9bLVMppVTZU+SgIyIuwLvA/UB7YJiItM+T7XHgsjHmT8BbwGuWY9sDQ4EOwH3AfBFxucUylVJKlTHF0dPpBBw3xpw0xtwAVgD98+TpD0Ra3q8C/izZt3T3B1YYY64bY34CjlvKu5UylVJKlTHFEXSaAL/abJ+ypNnNY4zJAJKAegUceytlKqWUKmPK/JRpERknItEiEn3+/HlnV0cppVQBiiPonAaa2Ww3taTZzSMilYHawMUCjr2VMgEwxiw0xgQZY4IaNGhQhGYopZQqacURdPYCrUTES0SqkD0xYF2ePOuAUZb3g4BtludyrwOGWma3eQGtgD23WKZSTme7wrRSqnBFDjqWazSTgc3AD8BnxphDIvKyiPSzZPsQqCcix4HpQLjl2EPAZ8Bh4D/AJGNMZn5lFrWuStmTkJBA27ZtGT58OO3atWPQoEFcu3aNFi1acOHCBQCio6Otqy9HREQwYsQIQkNDGTFiBIcOHaJTp074+/vj6+vLsWPHSEhIwNvb23qOuXPnEhERAcCJEyesD1rr2rUrR44ccXSTlXKaYrk51BizEdiYJ+1Fm/dpwOB8jp0NzL6VMpUqKUePHuXDDz8kNDSUMWPGMH/+/ALzHz58mB07duDm5sZTTz3F1KlTGT58ODdu3CAzMzPXI6zzGjduHAsWLKBVq1Z8//33TJw4kW3bthV3k5QqlXRFAlUh2T6sra5Jon7DxoSGhgLw17/+lXnz5hV4fL9+/XBzy14ItHPnzsyePZtTp2r1w1oAACAASURBVE4xcOBAWrVqle9xycnJ7Nq1i8GDf/8f7Pr168XQIqVKnogkAEHGmAsikmyMcf+jZZT52WtK/VE5D2s7nZiKAc5dSSPxWgZr9v0+V0VEqFy5MllZWQCkpaXlKqNGjRrW948++ijr1q3Dzc2NBx54gG3btuU61vb4rKwsPDw8iIuLs75++OGHEmytUqWLBh1V4eR9WBtAxpXfeHHhagA++eQTunTpQosWLYiJiQHgiy++yLe8kydP0rJlS6ZMmUL//v05cOAAnp6e/Pbbb1y8eJHr16+zfv16AGrVqoWXlxeff/45AMYY9u/fXxLNVBVQftcS582bR/v27fH19WXo0KFA9rXJuXPnWvOKSHzOEmUiskZEYkTkkIiMK+icIrJURAbYbC8XkXxv5tegoyqcM4mpN6VVrtuUk9+upl27dly+fJkJEybw0ksvMXXqVIKCgnBxcbFTUrbPPvsMb29v/P39iY+PZ+TIkbi6uvLiiy/SqVMnevXqRdu2ba35ly9fzocffoifnx8dOnRg7dq1JdJOpXLMmTOHffv2ceDAARYsWHArh4wxxgQCQcAUEalXQN4PgdEAIlIbuAfYkF9mvaajKpzGHm6czhN4pFIl/Ea8wM7wHta0rl278uOPP950fM4stBzh4eGEh9+8Ju2UKVOYMmXKTeleXl785z//uc3aK5Vb3uuTV9Junsbv6+vL8OHDGTBgAAMGDLBTyk2miMjDlvfNyL6d5aK9jMaY/4nIfBFpADwCfGGZgWyX9nRUhWPvYW0iog9rK0UeeOABEhMTSUxMzDWTMCoqigcffNCJNStdbro+eTWdc0nXrNcnc64lbtiwgUmTJhEbG0twcDAZGRk3XXcEqgGISBjQE+hsjPED9uXsK8BS4K/AY8DigjJq0FEVzoCAJrw60IcmHm4IcOedLVi5eac+R6cU2bhxIx4eHjcFHZVb3uuTLjU8yEhJ4p+r91ivJWZlZfHrr79y77338tprr5GUlERycjItWrQgNjY259DqgJflfW2ynwpwTUTaAiG3UJWPgL8BGGMOF5RRh9dUhaQPa3OuN954g6pVqzJlyhSmTZvG/v372bZtG9u2bePDDz9k586dREdHEx4ezokTJ/D396dXr1707duX5ORkBg0aRHx8PIGBgXz88cdkL1pf8eS9Pikulal9z1D2vTORXv9rRdu2bcnMzOSvf/0rSUlJGGOYMmUKHh4ePPLIIyxdupQOHToA3AHkjCX/BxgvIj8AR4HdhdXDGHPOkn9NYXk16ChVDn300UdER0fzzjvvOLsqdnXt2pU333yTKVOmEB0dzfXr10lPT2f79u1069aNnTt3AtkXwOPj44mLiwOyh9f27dvHoUOHaNw4+96qnTt30qVLF2c2x2nsXZ+sFdSPdj2H8K3N9Ul73Nzc+Prrr4Hs+2+MMUE2u++3d4wxpoXNe+s9OiJSnezrPp8WVmcdXlNKOcSafacJnbMNr/ANTPvvFbZ/t4crV65QtWpVOnfuTHR0NNu3b6dr164FltOpUyeaNm1KpUqV8Pf3JyEhwTENKIXsXZ90c3Vx6PVJEelJ9nJlbxtjkgrLr0FHqTJk6dKl+Pr64ufnx4gRI/jqq6+4++67CQgIoGfPnnaX3xk9ejQTJkwgJCSEli1bEhUVxZgxY2jXrh2jR4+25nN3d2fmzJn4+fkREhJiLev8+fM88sgjBAcHExwcbO2F/O9//8Pf3x9/f38CAgK4evUqZ8+epVu3bvj7++Pt7c327dvJzMykR7+/MKRPKHvfHEPS3jWcvZrOVdc6TH/l39xzzz107dqVb775huPHj9OuXbsCP4OqVata37u4uFToRVfzXp9s4uHGqwN9HDp0bIzZaoy50xjz71vJr8NrSpURhw4d4pVXXmHXrl3Ur1+fS5cuISLs3r0bEeGDDz7g9ddf580337zp2MuXL/Pdd9+xbt06+vXrx86dO/nggw8IDg4mLi4Of39/UlJSCAkJYfbs2TzzzDMsWrSI559/nqlTpzJt2jS6dOnCL7/8Qp8+ffjhhx+YO3cu7777LqGhoSQnJ1OtWjUWLlxInz59mDlzJpmZmVy7do24uDhifzhBozHvApCVlgyAa5P2LFv4LhtWLcfHx4fp06cTGBiY6/pMzZo1uXr1qmM+4DKqrF2f1KCjVBmxbds2Bg8eTP369QGoW7cuBw8eZMiQIZw9e5YbN27g5eVl99iHHnoIEcHHxwdPT098fHwA6NChAwkJCfj7+1OlShXrdOTAwEC2bNkCwNatWzl8+PcJSVeuXCE5OZnQ0FCmT5/O8OHDGThwIE2bNiU4OJgxY8aQnp7OgAED8Pf3p2XLlqRcOEPWlgW43RVMNa8AAKo27UDSd5/RuXNnatSoQbVq1W4aWqtXrx6hoaF4e3tz//3307dv3+L9UJXDadBRqhSzvfFPDv9Ixwa5Z2k99dRTTJ8+nX79+hEVFXXTjas5coakKlWqlGt4qlKlStbhKVdXV2svw3bYKisri927d1OtWu5bNcLDw+nbty8bN24kNDSUzZs3061bN7799ls2bNjA6NGjmT59OiNHjiTwbx9wIm4nV+M2kXJkO/Uf+BtuLfy5Z/bX1nXsbG/Etb1O88knn+Q6b84jJoBSO1FC5U+v6ShVSuW98S+tQTvWrVnN0m/iAbh06RJJSUk0aZI9tBIZGVki9ejduzdvv/22dTtnJtmJEyfw8fHh2WefJTg4mCNHjvDzzz/j6enJ2LFjeeKJJ4iNjeXChQtM6XEX9b274dH1r9w4dwJw/AVvVTpo0ClFPvroI86cOWPdfuKJJ3INa6iKJe+Nf1Ua3EmtkL8wfthD+Pn5MX36dCIiIhg8eDCBgYHWYbfiNm/ePKKjo/H19aV9+/bWtbv+/e9/4+3tja+vL66urtx///1ERUXh5+dHQEAAK1euZOrUqZw+fZq5Ux8l7bP/j6RNb1Gn2yinXPBWpYNkPzW6fAgKCjLR0dHOrsZtCwsLY+7cuQQFBRWeWZV7XuEbsPfbKcBPc/Tahio+IhKT5z6dEqM9nRKU32OQX375ZYKDg/H29mbcuHEYY1i1ahXR0dEMHz4cf39/UlNTCQsLIyeIfv3113Tu3JmOHTsyePBgkpOzZwC1aNGCl156iY4dO+Lj42N99HFycjKPPfYYPj4++Pr6Wpfmz68cVfo09nD7Q+lKlQUadErY0aNHmThxIj/88AO1atVi/vz5TJ48mb179xIfH09qairr169n0KBBBAUFsXz5cuLi4qxPpQS4cOECr7zyClu3biU2NpagoCD+9a9/WffXr1+f2NhYJkyYYH0+xqxZs6hduzYHDx7kwIED9OjRo9ByVOlSGm78q6hsb2QNnbMt1wP+VNEUKeiISF0R2SIixyxf6+STb5QlzzERGWWTHigiB0XkuIjME8vUGRGJEJHTIhJneT1QlHo6U7NmzXI9BnnHjh1888033H333fj4+LBt2zYOHTpUYBm7d+/m8OHDhIaG4u/vT2RkJD///LN1/8CBA4Hsaa45s362bt3KpEmTrHnq1KlTaDmqdCkNN/5VRHkncJxOTGX4wL58uLnsDt2XJkWdMh0O/NcYM0dEwi3bz9pmEJG6wEtkPwzIADEiss4Ycxl4DxgLfA9sBO4DNlkOfcsYM5cyJu+zLdLScy0djogwceJEoqOjadasGRERETc9CjkvYwy9evXi00/tL2uUMwW2sLuzCytHlT5l7ca/8iDvBA5jsrh+6QyL9pzn8T5OrFg5UdThtf5AzjzNSMDe04H6AFuMMZcsgWYLcJ+INAJqGWN2m+zZDEvzOb7MuOnZFlfSOP9/p5nz0Trg98cgQ/aQWHJyMqtWrbIen9/d1yEhIezcuZPjx48DkJKSYvfhYrZ69erFu+++a92+fPnybZWjVEWTd+Xm9Au/UL31PZxLycrnCPVHFDXoeBpjzlre/x/gaSdPE+BXm+1TlrQmlvd503NMFpEDIrI4v2E7ABEZJyLRIhJ9/vz522pEccn7HxJkPwb5zf83L9djkMeOHYu3tzd9+vQhODjYmnf06NGMHz/eOpEgR4MGDfjoo48YNmwYvr6+dO7c2TphID/PP/88ly9fxtvbGz8/P7755pvbKkepiibvRI0qDVpQ989jdQJHMSl0yrSIbAUa2tk1E4g0xnjY5L1sjMkVIETkaaCaMeYVy/YLQCoQBcwxxvS0pHcFnjXGPCginsAFsofjZgGNjDFjCmuMs6dM553impF0jt9W/YMmj8/XKa5KlRE5Ixa2/0C6ubqU6+tppWrKtDGmpzHG285rLXDOMkyG5etvdoo4TfYztnM0taSdtrzPm44x5pwxJtMYkwUsAjrdTuMcTae4KlX2GGPo0qULmzZlX04eENCEB2v+TNLqf+gEjhJQ1OG1dUDObLRRwFo7eTYDvUWkjmWYrDew2TIsd0VEQiyz1kbmHJ8TyCweBuKLWE+HyDvFtXJtT+4a/75OcVWqFBMRFixYwPTp00lLSyM5OZk1i+YSvelTfprTl53hPTTgFKMirUggIvWAz4DmwM/AX4wxl0QkCBhvjHnCkm8M8HfLYbONMUss6UFkP1vbjexZa08ZY4yILAP8yR5eSwCetLl2lC9nD69B7tlrjT3cmNGnjf7AKlUGPPPMM9SoUYOUlBRq1KjBvn37OHnyJNWrV2fhwoX4+voSERGBu7s7Tz/9NADe3t6sX7+eFi1aOLfyReTI4bUiTZk2xlwE/mwnPRp4wmZ7MbA4n3zedtJHFKVezqRTXJUqm3JW9qhSpQpdunQhICCANWvWsG3bNkaOHGld6FQVjT7aQClVYeUdmfDrfj9Bf2rMp59+al06qkePHly8eJErV644ubblgwYdpVSFlHeW2unEVI4euUD1GjXzPaZy5cpkZf1+v05hN3arm+naa0qpCsnefXUZWVlEHT1P165dWb58OQBRUVHUr1+fWrVq0aJFC2JjYwGIjY3lp59+cni9yzrt6SilKqS8Kw/kuJKaTkTEPxkzZgy+vr5Ur17d+oC8Rx55hKVLl9KhQwfuvvtuWrdu7cgqlwsadJRSFVJjDzdO5wk8Hl2G08TDjbp167JmzZqbjnFzc+Prr792VBXLJR1eK+WWLl2Kr68vfn5+jBgxgq+++oq7776bgIAAevbsyblz5wCIiIhgzJgxhIWF0bJlS+bNm+fkmitVuumjI5xDezql2KFDh3jllVfYtWsX9evX59KlS4gIu3fvRkT44IMPeP3113nzzTcBOHLkCN988w1Xr16lTZs2TJgwAVdXVye3QqnSKefWBr2vzrE06JRCOdM4j/z3M9yaBLPj1+sMqA9169bl4MGDDBkyhLNnz3Ljxg28vLysx/Xt25eqVatStWpV7rjjDs6dO0fTpk0LOJNSFZveV+d4OrxWyuR9PMLV6xk8t/qg9cmFTz31FJMnT+bgwYO8//77uaZs5jxXBwp/to5SSjmDBp1SxnYaZ7Xmvlw7soPkK5d5Y/NRLl26RFJSEk2aZP9nljOjRimlygodXitlbKdxVmlwJ7U7D+HcJ+Gck0pMP9KdiIgIBg8eTJ06dejRo4feJ6CUKlOKtOBnaVMaFvwsqtA5226axgnZy6vvDO/hhBoppcq7UvU8HeVYOo1TKVWe6fBaKaPTOJVS5ZkGnVJIp3EqpcorHV5TSinlMBp0lFJKOYwGHaWUUg5TpKAjInVFZIuIHLN8rZNPvlGWPMdEZJRN+mwR+VVEkvPkryoiK0XkuIh8LyItilJPpZRSpUNRezrhwH+NMa2A/1q2cxGRusBLwN1AJ+Alm+D0lSUtr8eBy8aYPwFvAa8VsZ5KKaVKgaIGnf5AzloskcAAO3n6AFuMMZeMMZeBLcB9AMaY3caYs4WUuwr4s4hIEeuqlFLKyYoadDxtgsb/AZ528jQBfrXZPmVJK4j1GGNMBpAE1CtaVZVSSjlboffpiMhWoKGdXTNtN4wxRkQcvqaOiIwDxgE0b97c0adXSin1BxQadIwxPfPbJyLnRKSRMeasiDQCfrOT7TQQZrPdFIgq5LSngWbAKRGpDNQGLuZTv4XAQshee62QcpVSSjlRUYfX1gE5s9FGAWvt5NkM9BaROpYJBL0tabda7iBgmylPK5MqpVQFVdSgMwfoJSLHgJ6WbUQkSEQ+ADDGXAJmAXstr5ctaYjI6yJyCqguIqdEJMJS7odAPRE5DkzHzqw4pZRSZY8+2kAppSo4fbSBUkqpckmDjlJKKYfRoKOUUsphNOgopZRyGA06SimlHEaDjlJKKYfRoKOUUsphNOgopZRyGA06SimlHEaDjlJKKYfRoKOUUsphNOgopZRyGA06SimlHEaDjlJKKYfRoKOUUsphNOgopZRyGA06SimlHEaDjlKlSEREBHPnzi215SlVVBp0lFJKOUyRgo6I1BWRLSJyzPK1Tj75RlnyHBORUTbps0XkVxFJzpN/tIicF5E4y+uJotRTqdJs9uzZtG7dmi5dunD06FEATpw4wX333UdgYCBdu3blyJEjJCUlceedd5KVlQVASkoKzZo1Iz093W7+vOLi4ggJCcHX15eHH36Yy5cvAxAWFsbUqVPx9/fH29ubPXv2OK7xqsIpak8nHPivMaYV8F/Ldi4iUhd4Cbgb6AS8ZBOcvrKk2bPSGONveX1QxHoqVSrFxMSwYsUK4uLi2LhxI3v37gVg3LhxvP3228TExDB37lwmTpxI7dq18ff353//+x8A69evp0+fPri6utrNn9fIkSN57bXXOHDgAD4+PvzjH/+w7rt27RpxcXHMnz+fMWPGOKbxqkKqXMTj+wNhlveRQBTwbJ48fYAtxphLACKyBbgP+NQYs9uSVsRqKFW2rNl3mjc2H+WHLSuocYc/Xx+9zICAJvTr14+0tDR27drF4MGDrfmvX78OwJAhQ1i5ciX33nsvK1asYOLEiSQnJ+ebP0dSUhKJiYl0794dgFGjRuXKP2zYMAC6devGlStXSExMxMPDo8TaryquogYdT2PMWcv7/wM87eRpAvxqs33KklaYR0SkG/AjMM0Y86u9TCIyDhgH0Lx581utt1JOs2bfaZ5bfZDU9EwArqZl8Nzqg9b9WVlZeHh4EBcXd9Ox/fr14+9//zuXLl0iJiaGHj16kJKSkm/+W5X3Hz/9R1CVlEKH10Rkq4jE23n1t81njDGAKaZ6fQW0MMb4AlvI7kXZZYxZaIwJMsYENWjQoJhOr1TJeWPzUWvAqdqsA9eO7Sbl2jXmrNvHV199RfXq1fHy8uLzzz8HwBjD/v37AXB3dyc4OJipU6fy4IMP4uLiQq1atfLNn6N27drUqVOH7du3A7Bs2TJrrwdg5cqVAOzYsYPatWtTu3btkv0QVIVVaE/HGNMzv30ick5EGhljzopII+A3O9lO8/sQHEBTsofhCjrnRZvND4DXC6unUmXFmcRU6/uqDf9EjbZdObvkKX6r7sHAbsEALF++nAkTJvDKK6+Qnp7O0KFD8fPzA7KH2AYPHkxUVJS1nILy54iMjGT8+PFcu3aNli1bsmTJEuu+atWqERAQQHp6OosXLy7B1quKTrI7KLd5sMgbwEVjzBwRCQfqGmOeyZOnLhADdLQkxQKBOdd4LHmSjTHuNtuNcobtRORh4FljTEhh9QkKCjLR0dG33R6lHCF0zjZO2wSeHE083NgZ3sPh9QkLC2Pu3LkEBQU5/NyqdBCRGGOMQ34Aijp7bQ7QS0SOAT0t24hIkIh8AGAJLrOAvZbXyzaTCl4XkVNAdRE5JSIRlnKniMghEdkPTAFGF7GeSpUaM/q0wc3VJVeam6sLM/q0cVKNlHKcIvV0Shvt6aiyImf22pnEVBp7uDGjTxsGBNzK/Bqlip8jezpFnb2mlLoNAwKaaJBRFZIug6PKFRcXF+ud9YMHD+batWskJCTg7e39h8r56KOPOHPmTAnV8ta4u7sXnkmpMkaDjipX3NzciIuLIz4+nipVqrBgwYLbKqc4g05GRkaxlKNUeaBBR5VbXbt25fjx4wBkZmYyduxYOnToQO/evUlNzZ49Zm89slWrVhEdHc3w4cPx9/cnNTWV//73vwQEBODj48OYMWOsd/xv3LiRtm3bEhgYyJQpU3jwwQeB7NWdR4wYQWhoKCNGjCAhIYGuXbvSsWNHOnbsyK5duwCIioqiW7du9O3blzZt2jB+/Hjr2moAM2fOxM/Pj5CQEM6dO8fVq1fx8vIiPT0dgCtXruTaVqq006CjyqWMjAw2bdqEj48PAMeOHWPSpEkcOnQIDw8PvvjiC8D+emSDBg0iKCiI5cuXExcXh4gwevRoVq5cycGDB8nIyOC9994jLS2NJ598kk2bNhETE8P58+dz1eHw4cNs3bqVTz/9lDvuuIMtW7YQGxvLypUrmTJlijXfnj17ePvttzl8+DAnTpxg9erVQPaCniEhIezfv59u3bqxaNEiatasSVhYGBs2bABgxYoVDBw4EFdXV0d8rEoVmQYdVa6kpqbi7+9PUFAQzZs35/HHHwfAy8sLf39/AAIDA0lISLC7Htm33357U5lHjx7Fy8uL1q1b58p35MgRWrZsiZeXF/D7+mU5+vXrh5ubGwDp6emMHTsWHx8fBg8ezOHDh635OnXqRMuWLXFxcWHYsGHs2LEDgCpVqlh7Tjl1BnjiiSesN3YuWbKExx57rOgfnFIOorPXVJlnO/2YylWIWLLhpplhVatWtb53cXGxDq+VpBo1aljfv/XWW3h6erJ//36ysrKoVq2adV9+6565urpa37u4uFivDYWGhpKQkEBUVBSZmZl/eJKEUs6kPR1VpuUsnnk6MRUDGAPPrT7Imn2nCz22oPXIatasydWrVwFo06YNCQkJ1utDOfnatGnDyZMnrT2QnPXL7ElKSqJRo0ZUqlSJZcuWkZmZad23Z88efvrpJ7Kysli5ciVdunQptO4jR47k0Ucf1V6OKnM06KgyzXbxzByp6Zm8sfnoLR0fGRnJjBkz8PX1JS4ujhdffBGA0aNHM378ePz9/THGsGTJEgYPHoyPjw+VKlVi/PjxuLm5MX/+fOvD02rWrJnvQpkTJ04kMjISPz8/jhw5kqsXFBwczOTJk2nXrh1eXl48/PDDhdZ7+PDhXL58+aYhPaVKO12RQJVpXuEb7C5tLsBPc/qW+PmTk5Nxd3fHGMOkSZNo1aoV06ZNu+Xjo6KimDt3LuvXr/9D5121ahVr165l2bJlf7TKSt1EVyRQ6hY19nCzu3hmYw83h5x/0aJFREZGcuPGDQICAnjyySdL/JxPPfUUmzZtYuPGjSV+LqWKm/Z0VJmW94FokL145qsDfXSZGaVukfZ0lLpFOYFFF89UqmzQoKPKPF08U6myQ2evKaWUchgNOkoppRxGg45SSimH0aCjlFLKYTToKKWUcphydZ+OiJwHfi4kW33gggOqU9LKQzvKQxugfLSjPLQBykc7nNGGO40xDRxxonIVdG6FiEQ76iaoklQe2lEe2gDlox3loQ1QPtpRHtpQEB1eU0op5TAadJRSSjlMRQw6C51dgWJSHtpRHtoA5aMd5aENUD7aUR7akK8Kd01HKaWU81TEno5SSiknKTdBR0TqisgWETlm+Vonn3yjLHmOicgom/TZIvKriCTnyT9aRM6LSJzl9UQZbENVEVkpIsdF5HsRaVFSbSimdgSKyEFLfeeJiFjSI0TktM334oESqPt9InLUcu5wO/vz/SxF5DlL+lER6XOrZZaEEmpHguX7EiciJf4Mkdttg4jUE5FvRCRZRN7Jc4zdn60y1oYoS5k5vwd3lGQbip0xply8gNeBcMv7cOA1O3nqAictX+tY3tex7AsBGgHJeY4ZDbxTxtswEVhgeT8UWFnK27HH0hYBNgH3W9IjgKdLsN4uwAmgJVAF2A+0v5XPEmhvyV8V8LKU43IrZZaFdlj2JQD1HfS7UJQ21AC6AOPz/u7m97NVxtoQBQQ54vtQEq9y09MB+gORlveRwAA7efoAW4wxl4wxl4EtwH0AxpjdxpizDqlp/kqqDbblrgL+XML/4d12O0SkEVDL0hYDLM3n+JLQCThujDlpjLkBrCC7Lbby+yz7AyuMMdeNMT8Bxy3l3UqZZaEdjnbbbTDGpBhjdgBptpmd8LNV7G0oD8pT0PG0+YP7f4CnnTxNgF9ttk9Z0grziIgcEJFVItKsiPUsSEm1wXqMMSYDSALqFa2qBSpKO5pY3udNzzHZ8r1YnN+wXRHcymeb32dZUHtu52euKEqiHQAG+FpEYkRkXAnU22797NTjpjy3+HNd2M9WcSuJNuRYYhlae6GkhwiLW5l6iJuIbAUa2tk103bDGGNEpLim5X0FfGqMuS4iT5L9X0mP2y3MSW0odk5qx3vALLL/+M0C3gTGFFPZqnBdjDGnLdcQtojIEWPMt86uVAU03PJ9qAl8AYwgu9dWJpSpoGOM6ZnfPhE5JyKNjDFnLd3o3+xkOw2E2Ww3JXt8tKBzXrTZ/IDs6xW3zRltsBzTDDglIpWB2sDFgg8pWAm247TlvW36acs5z9mcYxGw/nbrn4+cz+mmc9vJk/ezLOjYwsosbiXSDmNMztffRORLsoePSiroFKUNBZVp92erhJREG2y/D1dF5BOyvw9lJuiUp+G1dUDODKhRwFo7eTYDvUWkjmVoprclLV+WP5o5+gE/FENd81MibchT7iBgm2VMu6Tcdjssw3JXRCTEMmwwMuf4PN+Lh4H4Yq73XqCViHiJSBWyL+yuy5Mnv89yHTDUMhvJC2hF9kXrWymzuBV7O0SkhuU/a0SkBtnfr+L+/IurDXYV9LNVQoq9DfL/t3P/KBFDQQDGvwcK1mJjayXWewoR76DXsBEbwU6wtLP0Coq9YKGSwj8LnsDKWixmwBSihdlxI98PHlkeIdlJdhh4mU1rC621lfy8CGwx2/swvL/uSCLG6gAAAMNJREFUZBhqEOugl8ATcAEs5/wEOO3tt0s8HH0GdnrzR8Sa63tu93P+EOiIzpMrYH2EMSwB57n/NbA25/diQiTSFDjh80/MZ8A9cEck6+oMvvsm8Jjn3su5A2D7p2tJLC1OgQd6XVFfHbMgHwaNg+jAus3RVcTxyxhegFfgLXNh47vf1lhiILrabjIHOuCY7C4cy/CNBJKkMv9peU2SNOcsOpKkMhYdSVIZi44kqYxFR5JUxqIjSSpj0ZEklbHoSJLKfAC1N4uqx3pD6wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQShfmAdKLF"
      },
      "source": [
        "Exercice 31 : How to represent the document using Doc2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts= [\" Photography is an excellent hobby to pursue \",\n",
        "        \" Photographers usually develop patience, calmnesss\"\n",
        "        \" You can try Photography with any good mobile too\"]\n",
        "\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "def tagged_document(list_of_list_of_words):\n",
        "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
        "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
        "my_data = list(tagged_document(all_tokens))\n",
        "model=Doc2Vec(my_data)\n",
        "\n",
        "model.infer_vector(['photography','is','an',' excellent ','hobby ','to',' pursue '])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-TIbiL0wZ_A",
        "outputId": "db78a120-3348-48ea-88cc-574067a19420"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.8496400e-04,  2.8852806e-03,  3.3970799e-03, -4.3369378e-03,\n",
              "       -2.7048516e-03,  1.6194897e-03, -8.3942414e-04, -2.8626332e-03,\n",
              "        1.8518185e-04, -3.6915725e-03,  7.6159544e-04,  1.6459033e-03,\n",
              "       -1.6075737e-03,  2.1153553e-03, -2.1404598e-03, -2.8610644e-03,\n",
              "        1.1599294e-03,  3.1650993e-03, -2.5727199e-03,  1.9821476e-03,\n",
              "       -2.8891773e-03, -1.0684722e-03, -1.5390330e-03, -4.6891603e-03,\n",
              "        2.4733709e-03,  4.9745599e-03, -2.2961400e-03, -1.4769245e-03,\n",
              "       -4.6210480e-03, -3.6680875e-03, -1.5124555e-03, -1.1233590e-03,\n",
              "        4.6931789e-03, -1.5668027e-03,  3.5570834e-03, -1.9931295e-03,\n",
              "        2.0103380e-03, -1.9506781e-04,  4.5154942e-03, -3.6282740e-03,\n",
              "       -9.5314829e-04,  1.0489111e-03, -1.8704470e-03, -1.2522541e-03,\n",
              "        1.8446768e-03, -9.7849313e-04,  7.8355800e-04, -4.0517268e-03,\n",
              "       -2.3370364e-03, -4.0644272e-03, -4.9117352e-03, -2.2577585e-03,\n",
              "       -1.7677080e-03, -8.2954741e-04,  2.9541769e-03,  5.4623251e-04,\n",
              "        3.4098737e-03, -4.1643879e-03,  9.2235103e-04, -2.6144721e-03,\n",
              "        2.6860591e-03, -4.5869243e-03,  4.9505285e-03, -3.2707728e-03,\n",
              "        3.8502770e-03,  3.2275280e-03,  3.8895428e-03, -4.5457589e-03,\n",
              "        2.1114622e-05,  4.9157855e-03, -2.2946778e-03,  4.7047837e-03,\n",
              "        2.4364570e-03,  4.5981109e-03,  4.4710482e-03, -3.7878770e-03,\n",
              "        4.9257879e-03,  3.3445805e-03, -1.7515022e-03,  1.8918042e-03,\n",
              "       -3.7624219e-03,  1.5619857e-04, -4.9956038e-04, -2.3406919e-03,\n",
              "        2.5947730e-03,  3.6730480e-03, -4.7027762e-03,  2.1175412e-04,\n",
              "        1.8502804e-03,  2.2211655e-03, -1.7352898e-03,  1.9376875e-03,\n",
              "        2.1264448e-03, -1.3995536e-04,  4.9024983e-03,  3.7742069e-03,\n",
              "        1.1641744e-03, -3.6860907e-03, -3.1735541e-03, -4.1434201e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1rNvuJwdM9Y"
      },
      "source": [
        "Exercice 32 : How to extract the TF-IDF Matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_documents=['Painting is a hobby for many , passion for some',\n",
        "                'My hobby is coin collection'\n",
        "                'I do some Painting every now and then']\n",
        "\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "doc_tokenized = [simple_preprocess(text) for text in text_documents]\n",
        "dictionary = corpora.Dictionary()\n",
        "\n",
        "\n",
        "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
        "for doc in BoW_corpus:\n",
        "   print([[dictionary[id], freq] for id, freq in doc])\n",
        "import numpy as np\n",
        "tfidf = models.TfidfModel(BoW_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "WT8mT5j_wpUj",
        "outputId": "7617534d-48d4-4d2d-8243-937517f1ad41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['for', 2], ['hobby', 1], ['is', 1], ['many', 1], ['painting', 1], ['passion', 1], ['some', 1]]\n",
            "[['hobby', 1], ['is', 1], ['painting', 1], ['some', 1], ['and', 1], ['coin', 1], ['collectioni', 1], ['do', 1], ['every', 1], ['my', 1], ['now', 1], ['then', 1]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-40acd17623a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBoW_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W8soQyndOR7"
      },
      "source": [
        "Exercice 33 : How to create bigrams using Gensim’s Phraser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"the mayor of new york was there\", \"new york mayor was present\"]\n",
        "\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "sentence_stream = [doc.split(\" \") for doc in documents]\n",
        "\n",
        "\n",
        "bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
        "bigram_phraser = Phraser(bigram)\n",
        "\n",
        "for sent in sentence_stream:\n",
        "    tokens_ = bigram_phraser[sent]\n",
        "    print(tokens_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkb0RWMUwzP4",
        "outputId": "9e81ed87-13c8-4338-edb9-7df0ec2ed6e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'mayor', 'of', 'new york', 'was', 'there']\n",
            "['new york', 'mayor', 'was', 'present']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyBj-Em9dPeV"
      },
      "source": [
        "Exercice 34 : How to create bigrams, trigrams using ngrams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentences=\"Machine learning is a neccessary field in today's world. Data science can do wonders . Natural Language Processing is how machines understand text \"\n",
        "\n",
        "from nltk import ngrams\n",
        "bigram=list(ngrams(Sentences.lower().split(),2))\n",
        "trigram=list(ngrams(Sentences.lower().split(),3))\n",
        "\n",
        "print(\" Bigrams are\",bigram)\n",
        "print(\" Trigrams are\", trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP02e5huw9ra",
        "outputId": "a3e3100f-d42a-48e8-8a2b-bfed59c763be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bigrams are [('machine', 'learning'), ('learning', 'is'), ('is', 'a'), ('a', 'neccessary'), ('neccessary', 'field'), ('field', 'in'), ('in', \"today's\"), (\"today's\", 'world.'), ('world.', 'data'), ('data', 'science'), ('science', 'can'), ('can', 'do'), ('do', 'wonders'), ('wonders', '.'), ('.', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'how'), ('how', 'machines'), ('machines', 'understand'), ('understand', 'text')]\n",
            " Trigrams are [('machine', 'learning', 'is'), ('learning', 'is', 'a'), ('is', 'a', 'neccessary'), ('a', 'neccessary', 'field'), ('neccessary', 'field', 'in'), ('field', 'in', \"today's\"), ('in', \"today's\", 'world.'), (\"today's\", 'world.', 'data'), ('world.', 'data', 'science'), ('data', 'science', 'can'), ('science', 'can', 'do'), ('can', 'do', 'wonders'), ('do', 'wonders', '.'), ('wonders', '.', 'natural'), ('.', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'how'), ('is', 'how', 'machines'), ('how', 'machines', 'understand'), ('machines', 'understand', 'text')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYtXyVmadRHP"
      },
      "source": [
        "Exercice 35 : How to detect the language of entered text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"El agente imprime su pase de abordaje. Los oficiales de seguridad del aeropuerto pasan junto a él con un perro grande. El perro está olfateando alrededor del equipaje de las personas tratando de detectar drogas o explosivos.\"\n",
        "\n",
        "import spacy\n",
        "!pip install spacy_langdetect\n",
        "from spacy_langdetect import LanguageDetector\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(doc._.language)\n",
        "\n",
        "for sent in doc.sents:\n",
        "   print(sent, sent._.language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "IMJZ-A0bxHgc",
        "outputId": "02418726-fb74-4946-f61e-cb33b21e1478"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_langdetect\n",
            "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spacy_langdetect) (3.6.4)\n",
            "Collecting langdetect==1.0.7\n",
            "  Downloading langdetect-1.0.7.zip (998 kB)\n",
            "\u001b[K     |████████████████████████████████| 998 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect==1.0.7->spacy_langdetect) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.4.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (22.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy_langdetect) (8.13.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993431 sha256=09783566d12949d6d77f5854b34a5617ace8e1fa39a1547579aaa3a8dd00ebe0\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/79/3b/9885ae7f4308f73c514f96d8574d40d7d8173a27731b674013\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy-langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-850a3d316cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install spacy_langdetect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy_langdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguageDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMgSKKDEdSaq"
      },
      "source": [
        "Exercice 36 : How to merge two tokens as one"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Robert Langdon is a famous character in various books and movies \"\n",
        "\n",
        "\n",
        "doc = nlp(text)\n",
        "with doc.retokenize() as retokenizer:\n",
        "    retokenizer.merge(doc[0:14])\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "qQ3jgMMvxZde",
        "outputId": "a0684843-264a-44ca-ead0-090ecc6aca91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e4f519389163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mretokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mretokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP-Exercices.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtSCsggGuFDwT5n1APZolJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}